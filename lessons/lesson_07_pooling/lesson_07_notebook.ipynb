{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7: Graph Pooling & Hierarchical GNNs\n",
    "\n",
    "## Practical Implementation and Experiments\n",
    "\n",
    "In this notebook, we'll implement and experiment with various graph pooling techniques:\n",
    "1. Global pooling operations\n",
    "2. Hierarchical pooling\n",
    "3. DiffPool\n",
    "4. SAGPool\n",
    "5. Graph classification tasks\n",
    "6. Molecular property prediction\n",
    "7. Visualization of pooled graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.data import Data, DataLoader as GeoDataLoader, Batch\n",
    "    from torch_geometric.nn import GCNConv, GlobalMeanPool, GlobalMaxPool, global_mean_pool, global_max_pool, global_add_pool\n",
    "    from torch_geometric.utils import to_networkx, subgraph\n",
    "    import torch_geometric.transforms as T\n",
    "except ImportError:\n",
    "    print(\"PyTorch Geometric not installed. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'torch-geometric'])\n",
    "    from torch_geometric.data import Data, DataLoader as GeoDataLoader, Batch\n",
    "    from torch_geometric.nn import GCNConv, GlobalMeanPool, GlobalMaxPool, global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Global Pooling Operations\n",
    "\n",
    "Let's start by implementing and understanding basic pooling operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalPooling(nn.Module):\n",
    "    \"\"\"Implements different global pooling operations.\"\"\"\n",
    "\n",
    "    def __init__(self, method: str = 'mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            method: 'sum', 'mean', 'max', or 'concat' (concatenates all three)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert method in ['sum', 'mean', 'max', 'concat']\n",
    "        self.method = method\n",
    "\n",
    "    def forward(self, x: torch.Tensor, batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features [num_nodes, num_features]\n",
    "            batch: Batch assignment tensor [num_nodes] - which graph each node belongs to\n",
    "\n",
    "        Returns:\n",
    "            Graph-level embeddings [num_graphs, num_features or 3*num_features]\n",
    "        \"\"\"\n",
    "        if self.method == 'sum':\n",
    "            return global_add_pool(x, batch)\n",
    "        elif self.method == 'mean':\n",
    "            return global_mean_pool(x, batch)\n",
    "        elif self.method == 'max':\n",
    "            return global_max_pool(x, batch)\n",
    "        else:  # concat\n",
    "            sum_pool = global_add_pool(x, batch)\n",
    "            mean_pool = global_mean_pool(x, batch)\n",
    "            max_pool = global_max_pool(x, batch)\n",
    "            return torch.cat([sum_pool, mean_pool, max_pool], dim=1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GlobalPooling(method='{self.method}')\"\n",
    "\n",
    "\n",
    "# Test the pooling operations\n",
    "print(\"Global Pooling Operations\\n\" + \"=\"*50)\n",
    "\n",
    "# Create sample node embeddings from 3 graphs\n",
    "# Graph 1: 4 nodes, Graph 2: 3 nodes, Graph 3: 5 nodes\n",
    "x = torch.tensor([\n",
    "    # Graph 1\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0],\n",
    "    [7.0, 8.0],\n",
    "    # Graph 2\n",
    "    [2.0, 3.0],\n",
    "    [4.0, 5.0],\n",
    "    [6.0, 7.0],\n",
    "    # Graph 3\n",
    "    [1.0, 1.0],\n",
    "    [2.0, 2.0],\n",
    "    [3.0, 3.0],\n",
    "    [4.0, 4.0],\n",
    "    [5.0, 5.0],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Batch assignment: [0,0,0,0, 1,1,1, 2,2,2,2,2]\n",
    "batch = torch.tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2])\n",
    "\n",
    "# Test different pooling methods\n",
    "for method in ['sum', 'mean', 'max', 'concat']:\n",
    "    pool = GlobalPooling(method=method)\n",
    "    result = pool(x, batch)\n",
    "    print(f\"\\n{method.upper()} Pooling:\")\n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    print(f\"  Output shape: {result.shape}\")\n",
    "    print(f\"  Output:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Pooling Effects\n",
    "\n",
    "Let's visualize how different pooling methods aggregate information differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pooling_effects():\n",
    "    \"\"\"Visualize how different pooling methods affect information aggregation.\"\"\"\n",
    "\n",
    "    # Create sample feature vectors with different characteristics\n",
    "    features = {\n",
    "        'Uniform': torch.ones(10, 3),\n",
    "        'Sparse': torch.tensor(\n",
    "            [[1, 0, 0]] * 5 + [[0, 0, 0]] * 5,\n",
    "            dtype=torch.float32\n",
    "        ),\n",
    "        'Varied': torch.tensor(\n",
    "            [[i * 0.5, i * 0.3, i * 0.7] for i in range(10)],\n",
    "            dtype=torch.float32\n",
    "        ),\n",
    "        'Outliers': torch.tensor(\n",
    "            [[1, 1, 1]] * 9 + [[100, 100, 100]],\n",
    "            dtype=torch.float32\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    batch = torch.zeros(10, dtype=torch.long)\n",
    "\n",
    "    fig, axes = plt.subplots(len(features), 4, figsize=(14, 3 * len(features)))\n",
    "\n",
    "    for row, (name, feat) in enumerate(features.items()):\n",
    "        # Original features\n",
    "        axes[row, 0].imshow(feat.numpy(), cmap='viridis', aspect='auto')\n",
    "        axes[row, 0].set_title(f'{name}: Original Features')\n",
    "        axes[row, 0].set_xlabel('Feature Dimension')\n",
    "        axes[row, 0].set_ylabel('Node')\n",
    "\n",
    "        # Sum pooling\n",
    "        sum_pool = global_add_pool(feat, batch)\n",
    "        axes[row, 1].bar(range(3), sum_pool[0].detach().numpy())\n",
    "        axes[row, 1].set_title(f'{name}: Sum Pooling')\n",
    "        axes[row, 1].set_ylabel('Aggregated Value')\n",
    "\n",
    "        # Mean pooling\n",
    "        mean_pool = global_mean_pool(feat, batch)\n",
    "        axes[row, 2].bar(range(3), mean_pool[0].detach().numpy())\n",
    "        axes[row, 2].set_title(f'{name}: Mean Pooling')\n",
    "        axes[row, 2].set_ylabel('Aggregated Value')\n",
    "\n",
    "        # Max pooling\n",
    "        max_pool = global_max_pool(feat, batch)\n",
    "        axes[row, 3].bar(range(3), max_pool[0].detach().numpy())\n",
    "        axes[row, 3].set_title(f'{name}: Max Pooling')\n",
    "        axes[row, 3].set_ylabel('Aggregated Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary table\n",
    "    print(\"\\nPooling Methods Comparison\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"{'Method':<15} {'Uniform':<20} {'Sparse':<20} {'Varied':<20} {'Outliers':<20}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for method_name, method_func in [\n",
    "        ('Sum', global_add_pool),\n",
    "        ('Mean', global_mean_pool),\n",
    "        ('Max', global_max_pool),\n",
    "    ]:\n",
    "        results = []\n",
    "        for name, feat in features.items():\n",
    "            batch = torch.zeros(len(feat), dtype=torch.long)\n",
    "            pooled = method_func(feat, batch)\n",
    "            # Use first feature dimension for comparison\n",
    "            results.append(f\"{pooled[0, 0].item():.2f}\")\n",
    "\n",
    "        print(f\"{method_name:<15} {results[0]:<20} {results[1]:<20} {results[2]:<20} {results[3]:<20}\")\n",
    "\n",
    "\n",
    "visualize_pooling_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implementing Hierarchical Pooling\n",
    "\n",
    "Now let's implement a learnable hierarchical pooling layer (Top-K pooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Top-K Pooling layer.\n",
    "    Selects the top-k nodes based on learned importance scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, ratio: float = 0.8, nonlinearity=torch.sigmoid):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Size of node features\n",
    "            ratio: Pooling ratio (fraction of nodes to keep)\n",
    "            nonlinearity: Nonlinearity for scoring (sigmoid or tanh)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        # Learnable scoring weights\n",
    "        self.weight = nn.Parameter(torch.ones(in_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.view(-1, 1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        batch: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Edge indices [2, num_edges]\n",
    "            batch: Batch assignment [num_nodes]\n",
    "\n",
    "        Returns:\n",
    "            x_pool: Pooled node features\n",
    "            edge_index_pool: Pooled edge indices\n",
    "            batch_pool: Pooled batch assignment\n",
    "        \"\"\"\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "\n",
    "        # Compute importance scores\n",
    "        scores = torch.sigmoid(torch.sum(x * self.weight, dim=1))\n",
    "\n",
    "        # Determine number of nodes to keep\n",
    "        num_nodes = x.size(0)\n",
    "        num_keep = int(self.ratio * num_nodes)\n",
    "        num_keep = max(1, num_keep)  # Keep at least 1 node\n",
    "\n",
    "        # Select top-k nodes\n",
    "        keep_idx = torch.topk(scores, k=num_keep)[1]\n",
    "        keep_mask = torch.zeros(num_nodes, dtype=torch.bool, device=x.device)\n",
    "        keep_mask[keep_idx] = True\n",
    "\n",
    "        # Pool node features\n",
    "        x_pool = x[keep_idx]\n",
    "\n",
    "        # Pool batch assignment\n",
    "        batch_pool = batch[keep_idx]\n",
    "\n",
    "        # Pool edge indices (keep edges between kept nodes)\n",
    "        mask = keep_mask[edge_index[0]] & keep_mask[edge_index[1]]\n",
    "        edge_index_pool = edge_index[:, mask]\n",
    "\n",
    "        # Remap node indices\n",
    "        node_idx_mapping = torch.full((num_nodes,), -1, dtype=torch.long, device=x.device)\n",
    "        node_idx_mapping[keep_idx] = torch.arange(num_keep, device=x.device)\n",
    "        edge_index_pool = node_idx_mapping[edge_index_pool]\n",
    "\n",
    "        return x_pool, edge_index_pool, batch_pool\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TopKPooling(in_channels={self.in_channels}, ratio={self.ratio})\"\n",
    "\n",
    "\n",
    "# Test Top-K Pooling\n",
    "print(\"Top-K Pooling Implementation\\n\" + \"=\"*50)\n",
    "\n",
    "# Create a simple graph\n",
    "x = torch.randn(10, 8)\n",
    "edge_index = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 2, 4, 6, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 3, 5, 7, 9, 1],\n",
    "    ],\n",
    "    dtype=torch.long,\n",
    ")\nbatch = torch.zeros(10, dtype=torch.long)\n",
    "\n",
    "pool_layer = TopKPooling(in_channels=8, ratio=0.7)\n",
    "x_pool, edge_index_pool, batch_pool = pool_layer(x, edge_index, batch)\n",
    "\n",
    "print(f\"Original graph:\")\n",
    "print(f\"  Nodes: {x.shape[0]}, Edges: {edge_index.shape[1]}\")\n",
    "print(f\"\\nAfter Top-K Pooling (ratio=0.7):\")\n",
    "print(f\"  Nodes: {x_pool.shape[0]}, Edges: {edge_index_pool.shape[1]}\")\n",
    "print(f\"  Reduction: {x.shape[0]} → {x_pool.shape[0]} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Pooling Selection\n",
    "\n",
    "Let's visualize which nodes are selected by the Top-K pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_topk_selection():\n",
    "    \"\"\"Visualize which nodes are selected by Top-K pooling.\"\"\"\n",
    "    \n",
    "    # Create a simple synthetic graph\n",
    "    np.random.seed(42)\n",
    "    num_nodes = 20\n",
    "    \n",
    "    # Create node positions for visualization\n",
    "    pos = np.random.randn(num_nodes, 2)\n",
    "    \n",
    "    # Create edges\n",
    "    edges = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i+1, num_nodes):\n",
    "            # Connect nearby nodes\n",
    "            dist = np.linalg.norm(pos[i] - pos[j])\n",
    "            if dist < 1.5:\n",
    "                edges.append([i, j])\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create node features\n",
    "    x = torch.randn(num_nodes, 16)\n",
    "    batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    \n",
    "    # Apply pooling with different ratios\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    ratios = [1.0, 0.8, 0.6, 0.4, 0.2]\n",
    "    \n",
    "    for idx, ratio in enumerate(ratios):\n",
    "        ax = axes.flatten()[idx]\n",
    "        \n",
    "        # Apply pooling\n",
    "        pool = TopKPooling(in_channels=16, ratio=ratio)\n",
    "        x_pool, edge_index_pool, _ = pool(x, edge_index, batch)\n",
    "        \n",
    "        # Get selected node indices\n",
    "        num_keep = len(x_pool)\n",
    "        \n",
    "        # Create network graph\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(num_nodes))\n",
    "        \n",
    "        for edge in edge_index.t().numpy():\n",
    "            G.add_edge(edge[0], edge[1])\n",
    "        \n",
    "        # Draw\n",
    "        node_colors = ['lightblue' if i < num_keep else 'lightgray' \n",
    "                       for i in range(num_nodes)]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                              node_size=300, ax=ax)\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.3, ax=ax)\n",
    "        \n",
    "        ax.set_title(f'Pooling Ratio: {ratio:.1f}\\n({num_keep}/{num_nodes} nodes kept)')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Remove extra subplot\n",
    "    axes.flatten()[-1].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Blue nodes: Selected by Top-K Pooling\")\n",
    "    print(\"Gray nodes: Removed by pooling\")\n",
    "\n",
    "visualize_topk_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: SAGPool (Self-Attention Graph Pooling)\n",
    "\n",
    "Implement a more sophisticated pooling method using attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention Graph Pooling.\n",
    "    Uses attention-based scoring to select important nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, ratio: float = 0.8, GNN=GCNConv):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Size of input features\n",
    "            ratio: Pooling ratio\n",
    "            GNN: GNN layer to use for feature computation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "\n",
    "        # GNN for computing node importance\n",
    "        self.gnn = GNN(in_channels, 1)\n",
    "\n",
    "        # Scoring function\n",
    "        self.weight = nn.Parameter(torch.ones(in_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.view(-1, 1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        batch: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Edge indices [2, num_edges]\n",
    "            batch: Batch assignment [num_nodes]\n",
    "\n",
    "        Returns:\n",
    "            x_pool: Pooled node features\n",
    "            edge_index_pool: Pooled edge indices\n",
    "            batch_pool: Pooled batch assignment\n",
    "        \"\"\"\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "\n",
    "        # Compute attention scores using GNN\n",
    "        scores = self.gnn(x, edge_index).squeeze()\n",
    "        scores = torch.sigmoid(scores)\n",
    "\n",
    "        # Determine number of nodes to keep\n",
    "        num_nodes = x.size(0)\n",
    "        num_keep = int(self.ratio * num_nodes)\n",
    "        num_keep = max(1, num_keep)\n",
    "\n",
    "        # Select top-k nodes\n",
    "        keep_idx = torch.topk(scores, k=num_keep)[1]\n",
    "        keep_mask = torch.zeros(num_nodes, dtype=torch.bool, device=x.device)\n",
    "        keep_mask[keep_idx] = True\n",
    "\n",
    "        # Pool node features\n",
    "        x_pool = x[keep_idx]\n",
    "\n",
    "        # Pool batch assignment\n",
    "        batch_pool = batch[keep_idx]\n",
    "\n",
    "        # Pool edge indices\n",
    "        mask = keep_mask[edge_index[0]] & keep_mask[edge_index[1]]\n",
    "        edge_index_pool = edge_index[:, mask]\n",
    "\n",
    "        # Remap node indices\n",
    "        node_idx_mapping = torch.full((num_nodes,), -1, dtype=torch.long, device=x.device)\n",
    "        node_idx_mapping[keep_idx] = torch.arange(num_keep, device=x.device)\n",
    "        edge_index_pool = node_idx_mapping[edge_index_pool]\n",
    "\n",
    "        return x_pool, edge_index_pool, batch_pool, scores\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"SAGPooling(in_channels={self.in_channels}, ratio={self.ratio})\"\n",
    "\n",
    "\n",
    "# Test SAGPool\n",
    "print(\"SAGPool (Self-Attention Graph Pooling)\\n\" + \"=\"*50)\n",
    "\n",
    "x = torch.randn(10, 8)\n",
    "edge_index = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 2, 4, 6, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 3, 5, 7, 9, 1],\n",
    "    ],\n",
    "    dtype=torch.long,\n",
    ")\nbatch = torch.zeros(10, dtype=torch.long)\n",
    "\n",
    "pool_layer = SAGPooling(in_channels=8, ratio=0.7)\n",
    "x_pool, edge_index_pool, batch_pool, scores = pool_layer(x, edge_index, batch)\n",
    "\n",
    "print(f\"Original graph: {x.shape[0]} nodes, {edge_index.shape[1]} edges\")\n",
    "print(f\"After SAGPooling: {x_pool.shape[0]} nodes, {edge_index_pool.shape[1]} edges\")\n",
    "print(f\"\\nNode importance scores:\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"  Node {i}: {score.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Graph Classification Task\n",
    "\n",
    "Now let's build a complete GNN for graph classification using hierarchical pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Hierarchical GNN with multiple pooling layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        num_classes: int,\n",
    "        num_layers: int = 3,\n",
    "        pooling_ratio: float = 0.8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Input layer\n",
    "        self.embed = nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # GNN and pooling layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.global_pools = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.pools.append(TopKPooling(hidden_channels, ratio=pooling_ratio))\n",
    "            self.global_pools.append(GlobalPooling(method='concat'))\n",
    "\n",
    "        # Output layer\n",
    "        # 3 * hidden_channels from concatenated pooling (sum, mean, max)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3 * hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_channels, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features\n",
    "            edge_index: Edge indices\n",
    "            batch: Batch assignment\n",
    "\n",
    "        Returns:\n",
    "            Graph-level predictions\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # GNN + Pooling layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x, edge_index, batch = self.pools[i](x, edge_index, batch)\n",
    "\n",
    "        # Global pooling\n",
    "        x = self.global_pools[-1](x, batch)\n",
    "\n",
    "        # Readout\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"HierarchicalGNN(in_channels={self.in_channels}, \"\n",
    "            f\"hidden_channels={self.hidden_channels}, \"\n",
    "            f\"num_classes={self.num_classes}, \"\n",
    "            f\"num_layers={self.num_layers})\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Test the model\n",
    "print(\"Hierarchical GNN Model\\n\" + \"=\"*50)\n",
    "\n",
    "model = HierarchicalGNN(\n",
    "    in_channels=8,\n",
    "    hidden_channels=32,\n",
    "    num_classes=2,\n",
    "    num_layers=2,\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(10, 8)\n",
    "edge_index = torch.tensor(\n",
    "    [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 2, 4, 6, 8],\n",
    "     [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 3, 5, 7, 9, 1]],\n",
    "    dtype=torch.long,\n",
    ")\nbatch = torch.zeros(10, dtype=torch.long)\n",
    "\n",
    "output = model(x, edge_index, batch)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output logits: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Synthetic Graph Classification Dataset\n",
    "\n",
    "Create a synthetic graph classification dataset and train models on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_graph_dataset(num_graphs: int = 100, num_classes: int = 2):\n",
    "    \"\"\"\n",
    "    Create a synthetic graph dataset.\n",
    "    Class 0: Random graphs\n",
    "    Class 1: Graphs with strong community structure\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "\n",
    "    for graph_id in range(num_graphs):\n",
    "        if graph_id % 2 == 0:\n",
    "            # Class 0: Random graph\n",
    "            num_nodes = np.random.randint(10, 30)\n",
    "            p = 0.15\n",
    "            G = nx.erdos_renyi_graph(num_nodes, p)\n",
    "            label = 0\n",
    "        else:\n",
    "            # Class 1: Graph with community structure\n",
    "            num_nodes = np.random.randint(10, 30)\n",
    "            num_communities = np.random.randint(2, 4)\n",
    "            \n",
    "            # Create community structure\n",
    "            G = nx.Graph()\n",
    "            nodes_per_community = num_nodes // num_communities\n",
    "            node_idx = 0\n",
    "\n",
    "            for c in range(num_communities):\n",
    "                # Dense connections within community\n",
    "                for i in range(nodes_per_community):\n",
    "                    for j in range(i + 1, nodes_per_community):\n",
    "                        if np.random.random() < 0.7:\n",
    "                            G.add_edge(node_idx + i, node_idx + j)\n",
    "\n",
    "                # Few connections between communities\n",
    "                if c < num_communities - 1:\n",
    "                    for _ in range(2):\n",
    "                        u = np.random.randint(node_idx, node_idx + nodes_per_community)\n",
    "                        v = np.random.randint(\n",
    "                            node_idx + nodes_per_community,\n",
    "                            min(node_idx + 2 * nodes_per_community, num_nodes),\n",
    "                        )\n",
    "                        if v < num_nodes:\n",
    "                            G.add_edge(u, v)\n",
    "\n",
    "                node_idx += nodes_per_community\n",
    "\n",
    "            label = 1\n",
    "\n",
    "        if G.number_of_nodes() > 0 and G.number_of_edges() > 0:\n",
    "            # Convert to PyG Data\n",
    "            edge_index = torch.tensor(\n",
    "                list(G.edges()), dtype=torch.long\n",
    "            ).t().contiguous()\n",
    "            \n",
    "            # If no edges, skip\n",
    "            if edge_index.size(1) > 0:\n",
    "                # Add reverse edges (undirected)\n",
    "                edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)\n",
    "                edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "            # Node features: random features + degree\n",
    "            num_nodes = G.number_of_nodes()\n",
    "            degree = torch.tensor([G.degree(i) for i in range(num_nodes)], dtype=torch.float32)\n",
    "            x = torch.randn(num_nodes, 8)\n",
    "            x[:, 0] = degree  # Add degree as a feature\n",
    "\n",
    "            data = Data(\n",
    "                x=x,\n",
    "                edge_index=edge_index,\n",
    "                y=torch.tensor([label], dtype=torch.long),\n",
    "            )\n",
    "            graphs.append(data)\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating Synthetic Graph Classification Dataset\\n\" + \"=\"*50)\n",
    "dataset = create_synthetic_graph_dataset(num_graphs=100, num_classes=2)\n",
    "print(f\"Total graphs: {len(dataset)}\")\n",
    "\n",
    "# Dataset statistics\n",
    "num_nodes_list = [g.num_nodes for g in dataset]\n",
    "num_edges_list = [g.num_edges for g in dataset]\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Number of graphs: {len(dataset)}\")\n",
    "print(f\"  Number of nodes: min={min(num_nodes_list)}, max={max(num_nodes_list)}, avg={np.mean(num_nodes_list):.1f}\")\n",
    "print(f\"  Number of edges: min={min(num_edges_list)}, max={max(num_edges_list)}, avg={np.mean(num_edges_list):.1f}\")\n",
    "\n",
    "# Class distribution\n",
    "labels = [g.y.item() for g in dataset]\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Class 0 (Random): {labels.count(0)}\")\n",
    "print(f\"  Class 1 (Community): {labels.count(1)}\")\n",
    "\n",
    "# Visualize some graphs\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for idx, graph_idx in enumerate([0, 1, 10, 11, 20, 21]):\n",
    "    ax = axes.flatten()[idx]\n",
    "    \n",
    "    data = dataset[graph_idx]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(data.num_nodes))\n",
    "    for edge in data.edge_index.t().numpy():\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    label_text = \"Random\" if data.y.item() == 0 else \"Community\"\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_size=100, node_color='lightblue', ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, ax=ax)\n",
    "    ax.set_title(f\"{label_text} Graph (n={data.num_nodes}, m={data.num_edges//2})\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Graph Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_classifier(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = F.cross_entropy(out, batch.y.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == batch.y.view(-1)).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "\n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate_graph_classifier(model, loader, device):\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == batch.y.view(-1)).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# Train model\n",
    "print(\"Training Graph Classification Model\\n\" + \"=\"*50)\n",
    "\n",
    "device = torch.device('cpu')  # Use CPU for compatibility\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_set, val_set, test_set = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 8\n",
    "train_loader = GeoDataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = GeoDataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = GeoDataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "print(f\"Train size: {len(train_set)}, Val size: {len(val_set)}, Test size: {len(test_set)}\")\n",
    "\n",
    "# Create and train model\n",
    "model = HierarchicalGNN(\n",
    "    in_channels=8,\n",
    "    hidden_channels=32,\n",
    "    num_classes=2,\n",
    "    num_layers=2,\n",
    ")\nmodel = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\nTraining for {num_epochs} epochs...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_graph_classifier(model, train_loader, optimizer, device)\n",
    "    val_acc = evaluate_graph_classifier(model, val_loader, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:3d} | Loss: {train_loss:.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "# Test performance\n",
    "test_acc = evaluate_graph_classifier(model, test_loader, device)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses, label='Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accs, label='Training Accuracy')\n",
    "axes[1].plot(val_accs, label='Validation Accuracy')\n",
    "axes[1].axhline(y=test_acc, color='r', linestyle='--', label=f'Test Accuracy ({test_acc:.4f})')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Molecular Property Prediction\n",
    "\n",
    "Now let's work with a real molecular dataset using SMILES strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple molecular dataset\n",
    "# In practice, you would use RDKit to convert SMILES to graphs\n",
    "\n",
    "def create_molecule_dataset(num_molecules: int = 50):\n",
    "    \"\"\"\n",
    "    Create a synthetic molecular dataset.\n",
    "    In practice, this would use RDKit and real molecular data.\n",
    "    \"\"\"\n",
    "    molecules = []\n",
    "\n",
    "    for mol_id in range(num_molecules):\n",
    "        # Create random molecular-like graphs\n",
    "        num_atoms = np.random.randint(5, 20)\n",
    "        \n",
    "        # Create a random tree-like structure (like a molecule)\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(num_atoms))\n",
    "        \n",
    "        # Add edges to form connected structure\n",
    "        for i in range(1, num_atoms):\n",
    "            parent = np.random.randint(0, i)\n",
    "            G.add_edge(parent, i)\n",
    "        \n",
    "        # Randomly add extra edges\n",
    "        for _ in range(np.random.randint(0, 3)):\n",
    "            u = np.random.randint(0, num_atoms)\n",
    "            v = np.random.randint(0, num_atoms)\n",
    "            if u != v:\n",
    "                G.add_edge(u, v)\n",
    "        \n",
    "        # Create property label (simulate bioactivity)\n",
    "        # Higher connectivity → higher activity (simplified)\n",
    "        avg_degree = sum(dict(G.degree()).values()) / num_atoms\n",
    "        activity = 1 if avg_degree > 2.0 else 0\n",
    "        \n",
    "        # Convert to PyG Data\n",
    "        edge_index = torch.tensor(\n",
    "            list(G.edges()), dtype=torch.long\n",
    "        ).t().contiguous()\n",
    "        \n",
    "        if edge_index.size(1) > 0:\n",
    "            edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)\n",
    "            edge_index = torch.unique(edge_index, dim=1)\n",
    "        \n",
    "        # Atom features (atomic number, degree, etc.)\n",
    "        degree = torch.tensor([G.degree(i) for i in range(num_atoms)], dtype=torch.float32)\n",
    "        x = torch.randn(num_atoms, 8)\n",
    "        x[:, 0] = degree\n",
    "        \n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            y=torch.tensor([activity], dtype=torch.long),\n",
    "        )\n",
    "        molecules.append(data)\n",
    "    \n",
    "    return molecules\n",
    "\n",
    "\n",
    "# Create molecular dataset\n",
    "print(\"Creating Molecular Dataset\\n\" + \"=\"*50)\n",
    "mol_dataset = create_molecule_dataset(num_molecules=100)\n",
    "print(f\"Total molecules: {len(mol_dataset)}\")\n",
    "\n",
    "# Dataset statistics\n",
    "num_atoms_list = [g.num_nodes for g in mol_dataset]\n",
    "labels = [g.y.item() for g in mol_dataset]\n",
    "\n",
    "print(f\"\\nMolecular Dataset Statistics:\")\n",
    "print(f\"  Number of molecules: {len(mol_dataset)}\")\n",
    "print(f\"  Atoms per molecule: min={min(num_atoms_list)}, max={max(num_atoms_list)}, avg={np.mean(num_atoms_list):.1f}\")\n",
    "print(f\"  Active molecules: {labels.count(1)}\")\n",
    "print(f\"  Inactive molecules: {labels.count(0)}\")\n",
    "\n",
    "# Visualize some molecules\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for idx in range(6):\n",
    "    ax = axes.flatten()[idx]\n",
    "    \n",
    "    data = mol_dataset[idx]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(data.num_nodes))\n",
    "    for edge in data.edge_index.t().numpy():\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42, k=0.5)\n",
    "    activity = \"Active\" if data.y.item() == 1 else \"Inactive\"\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_size=150, node_color='lightcoral', ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, ax=ax)\n",
    "    ax.set_title(f\"{activity} Molecule (atoms={data.num_nodes})\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Molecular Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on molecular dataset\n",
    "print(\"Training on Molecular Dataset\\n\" + \"=\"*50)\n",
    "\n",
    "# Split dataset\n",
    "mol_train_size = int(0.7 * len(mol_dataset))\n",
    "mol_val_size = int(0.15 * len(mol_dataset))\n",
    "mol_test_size = len(mol_dataset) - mol_train_size - mol_val_size\n",
    "\n",
    "mol_train_set, mol_val_set, mol_test_set = random_split(\n",
    "    mol_dataset, [mol_train_size, mol_val_size, mol_test_size]\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "mol_train_loader = GeoDataLoader(mol_train_set, batch_size=8, shuffle=True)\n",
    "mol_val_loader = GeoDataLoader(mol_val_set, batch_size=8)\n",
    "mol_test_loader = GeoDataLoader(mol_test_set, batch_size=8)\n",
    "\n",
    "print(f\"Train: {len(mol_train_set)}, Val: {len(mol_val_set)}, Test: {len(mol_test_set)}\")\n",
    "\n",
    "# Create model\n",
    "mol_model = HierarchicalGNN(\n",
    "    in_channels=8,\n",
    "    hidden_channels=32,\n",
    "    num_classes=2,\n",
    "    num_layers=2,\n",
    ")\nmol_model = mol_model.to(device)\n",
    "mol_optimizer = Adam(mol_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Train\n",
    "num_epochs = 50\n",
    "mol_train_losses = []\n",
    "mol_train_accs = []\n",
    "mol_val_accs = []\n",
    "\n",
    "print(f\"\\nTraining for {num_epochs} epochs...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_graph_classifier(mol_model, mol_train_loader, mol_optimizer, device)\n",
    "    val_acc = evaluate_graph_classifier(mol_model, mol_val_loader, device)\n",
    "\n",
    "    mol_train_losses.append(train_loss)\n",
    "    mol_train_accs.append(train_acc)\n",
    "    mol_val_accs.append(val_acc)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:3d} | Loss: {train_loss:.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "# Test\n",
    "mol_test_acc = evaluate_graph_classifier(mol_model, mol_test_loader, device)\n",
    "print(f\"\\nTest Accuracy: {mol_test_acc:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(mol_train_losses, label='Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Molecular Dataset: Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(mol_train_accs, label='Training Accuracy')\n",
    "axes[1].plot(mol_val_accs, label='Validation Accuracy')\n",
    "axes[1].axhline(y=mol_test_acc, color='r', linestyle='--', label=f'Test Accuracy ({mol_test_acc:.4f})')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Molecular Dataset: Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Visualizing Pooled Graph Hierarchies\n",
    "\n",
    "Let's visualize what happens at different levels of hierarchical pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizableHierarchicalGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Hierarchical GNN that returns intermediate representations for visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        num_classes: int,\n",
    "        num_layers: int = 3,\n",
    "        pooling_ratio: float = 0.8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.global_pools = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.pools.append(TopKPooling(hidden_channels, ratio=pooling_ratio))\n",
    "            self.global_pools.append(GlobalPooling(method='concat'))\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3 * hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_channels, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, List]:\n",
    "        \"\"\"\n",
    "        Returns predictions and intermediate representations.\n",
    "        \"\"\"\n",
    "        x = self.embed(x)\n",
    "\n",
    "        intermediates = []\n",
    "        intermediates.append((x.clone().detach(), edge_index.clone().detach(), batch.clone().detach()))\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x, edge_index, batch = self.pools[i](x, edge_index, batch)\n",
    "            intermediates.append((x.clone().detach(), edge_index.clone().detach(), batch.clone().detach()))\n",
    "\n",
    "        x = self.global_pools[-1](x, batch)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x, intermediates\n",
    "\n",
    "\n",
    "# Create and test\n",
    "print(\"Visualizing Hierarchical Pooling\\n\" + \"=\"*50)\n",
    "\n",
    "viz_model = VisualizableHierarchicalGNN(\n",
    "    in_channels=8,\n",
    "    hidden_channels=32,\n",
    "    num_classes=2,\n",
    "    num_layers=3,\n",
    "    pooling_ratio=0.7,\n",
    ")\nviz_model.eval()\n",
    "\n",
    "# Create a test graph\n",
    "test_graph = mol_dataset[0]\n",
    "x = test_graph.x.unsqueeze(0)  # Add batch dimension\n",
    "batch = torch.zeros(test_graph.num_nodes, dtype=torch.long)\n",
    "edge_index = test_graph.edge_index\n",
    "\n",
    "# Forward pass to get intermediate representations\n",
    "with torch.no_grad():\n",
    "    output, intermediates = viz_model(x.squeeze(0), edge_index, batch)\n",
    "\n",
    "# Visualize hierarchy\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for layer, (x_layer, edge_layer, batch_layer) in enumerate(intermediates):\n",
    "    # Create networkx graph\n",
    "    G = nx.Graph()\n",
    "    num_nodes = x_layer.shape[0]\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "    for edge in edge_layer.t().numpy():\n",
    "        if edge[0] < num_nodes and edge[1] < num_nodes:\n",
    "            G.add_edge(edge[0], edge[1])\n",
    "\n",
    "    # Draw\n",
    "    if G.number_of_nodes() > 0:\n",
    "        pos = nx.spring_layout(G, seed=42, k=0.5)\n",
    "        \n",
    "        # Node colors based on features\n",
    "        node_colors = x_layer[:, 0].numpy()  # Use first feature\n",
    "        \n",
    "        nodes = nx.draw_networkx_nodes(\n",
    "            G, pos, node_color=node_colors, node_size=200,\n",
    "            cmap='viridis', ax=axes[layer]\n",
    "        )\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.3, ax=axes[layer])\n",
    "        \n",
    "        axes[layer].set_title(\n",
    "            f\"Layer {layer}\\n({num_nodes} nodes, {G.number_of_edges()} edges)\"\n",
    "        )\n",
    "        axes[layer].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHierarchical Pooling Summary:\")\n",
    "print(\"=\"*50)\n",
    "for layer, (x_layer, edge_layer, batch_layer) in enumerate(intermediates):\n",
    "    print(f\"Layer {layer}: {x_layer.shape[0]} nodes, {edge_layer.shape[1]//2} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Comparing Pooling Methods\n",
    "\n",
    "Let's compare the performance and properties of different pooling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNNWithGlobalPool(nn.Module):\n",
    "    \"\"\"GNN with global pooling only (baseline).\"\"\"\n\n    def __init__(self, in_channels: int, hidden_channels: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Linear(in_channels, hidden_channels)\n        self.conv = GCNConv(hidden_channels, hidden_channels)\n        self.pool = GlobalPooling(method='concat')\n        self.mlp = nn.Sequential(\n            nn.Linear(3 * hidden_channels, hidden_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_channels, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor) -> torch.Tensor:\n        x = self.embed(x)\n        x = self.conv(x, edge_index)\n        x = F.relu(x)\n        x = self.pool(x, batch)\n        return self.mlp(x)\n\n\nclass GNNWithTopKPool(nn.Module):\n    \"\"\"GNN with Top-K pooling.\"\"\"\n\n    def __init__(self, in_channels: int, hidden_channels: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Linear(in_channels, hidden_channels)\n        self.conv1 = GCNConv(hidden_channels, hidden_channels)\n        self.pool = TopKPooling(hidden_channels, ratio=0.7)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.global_pool = GlobalPooling(method='concat')\n        self.mlp = nn.Sequential(\n            nn.Linear(3 * hidden_channels, hidden_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_channels, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor) -> torch.Tensor:\n        x = self.embed(x)\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, batch = self.pool(x, edge_index, batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = self.global_pool(x, batch)\n        return self.mlp(x)\n\n\nprint(\"Comparing Pooling Methods\\n\" + \"=\"*50)\n\nmodels_to_compare = {\n    'Global Pool': SimpleGNNWithGlobalPool(8, 32, 2),\n    'Top-K Pool': GNNWithTopKPool(8, 32, 2),\n    'Hierarchical': HierarchicalGNN(8, 32, 2, num_layers=2),\n}\n\n# Train each model\nresults = {}\nfor model_name, model in models_to_compare.items():\n    print(f\"\\nTraining {model_name}...\")\n    model = model.to(device)\n    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n\n    train_losses = []\n    val_accs = []\n\n    for epoch in range(30):\n        train_loss, _ = train_graph_classifier(model, mol_train_loader, optimizer, device)\n        val_acc = evaluate_graph_classifier(model, mol_val_loader, device)\n        train_losses.append(train_loss)\n        val_accs.append(val_acc)\n\n    test_acc = evaluate_graph_classifier(model, mol_test_loader, device)\n\n    # Count parameters\n    num_params = sum(p.numel() for p in model.parameters())\n\n    results[model_name] = {\n        'val_accs': val_accs,\n        'test_acc': test_acc,\n        'num_params': num_params,\n    }\n\n    print(f\"  Test Accuracy: {test_acc:.4f}, Parameters: {num_params:,}\")\n\n# Visualize comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\n# Validation accuracy comparison\nfor model_name, result in results.items():\n    axes[0].plot(result['val_accs'], label=model_name, marker='o', markersize=3)\n\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Validation Accuracy')\naxes[0].set_title('Validation Accuracy Comparison')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Test accuracy and parameter count\nmodel_names = list(results.keys())\ntest_accs = [results[m]['test_acc'] for m in model_names]\nnum_params = [results[m]['num_params'] for m in model_names]\n\nax2 = axes[1]\nax3 = ax2.twinx()\n\ncolors = ['C0', 'C1', 'C2']\nx_pos = np.arange(len(model_names))\n\nbars1 = ax2.bar(x_pos - 0.2, test_accs, 0.4, label='Test Accuracy', color=colors)\nax2.set_ylabel('Test Accuracy', color=colors[0])\nax2.set_ylim([0.4, 1.0])\n\nbars2 = ax3.bar(x_pos + 0.2, [p/1000 for p in num_params], 0.4, label='Parameters (K)', color=colors)\nax3.set_ylabel('Parameters (Thousands)', color=colors[1])\n\nax2.set_xticks(x_pos)\nax2.set_xticklabels(model_names)\nax2.set_title('Test Accuracy vs Model Complexity')\n\nfig.tight_layout()\nplt.show()\n\n# Print comparison table\nprint(\"\\n\" + \"=\"*70)\nprint(\"Model Comparison Summary\")\nprint(\"=\"*70)\nprint(f\"{'Model':<20} {'Test Accuracy':<20} {'Parameters':<20}\")\nprint(\"-\"*70)\nfor model_name in model_names:\n    test_acc = results[model_name]['test_acc']\n    num_params = results[model_name]['num_params']\n    print(f\"{model_name:<20} {test_acc:<20.4f} {num_params:<20,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Global Pooling Comparison\n",
    "Implement a comparison of all global pooling methods (sum, mean, max) on different graph size distributions.\n",
    "\n",
    "### Exercise 2: Custom Pooling Ratio\n",
    "Modify the Top-K pooling layer to have different pooling ratios at different layers. Train a model and compare results.\n",
    "\n",
    "### Exercise 3: SAGPool Implementation\n",
    "Complete the SAGPool implementation to use neighborhood information for scoring, not just individual node features.\n",
    "\n",
    "### Exercise 4: Visualization Analysis\n",
    "Create a function that visualizes the hierarchical clustering created by pooling layers.\n",
    "\n",
    "### Exercise 5: Large Graph Scalability\n",
    "Test pooling methods on increasingly larger graphs and measure runtime complexity.\n",
    "\n",
    "### Exercise 6: Hyperparameter Search\n",
    "Implement a grid search over pooling ratios and number of hierarchical layers.\n",
    "\n",
    "### Exercise 7: Real Molecular Data\n",
    "Use RDKit to convert SMILES strings to molecular graphs and train a model for property prediction.\n",
    "\n",
    "### Exercise 8: Attention Visualization\n",
    "Implement and visualize attention weights in SAGPool to show which nodes are important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
