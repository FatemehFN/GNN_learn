{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Graph Convolutional Networks (GCNs)\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Implement GCN layer from scratch\n",
    "2. Use PyTorch Geometric's built-in GCNConv\n",
    "3. Apply GCN to node classification on real datasets\n",
    "4. Visualize learned embeddings\n",
    "5. Analyze layer-by-layer behavior\n",
    "6. Tune hyperparameters\n",
    "7. Solve practical exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Graph Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple graph\n",
    "edge_list = torch.tensor([\n",
    "    [0, 1], [1, 0], [1, 2], [2, 1],\n",
    "    [0, 3], [3, 0], [1, 4], [4, 1],\n",
    "    [2, 5], [5, 2], [3, 4], [4, 3],\n",
    "    [4, 5], [5, 4],\n",
    "], dtype=torch.long).t().contiguous()\n",
    "\n",
    "n_nodes = 6\n",
    "n_features = 3\n",
    "x = torch.randn(n_nodes, n_features)\n",
    "graph_data = Data(x=x, edge_index=edge_list)\n",
    "\n",
    "print(f\"Number of nodes: {graph_data.num_nodes}\")\n",
    "print(f\"Number of edges: {graph_data.num_edges}\")\n",
    "print(f\"Node features shape: {graph_data.x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Graph Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index_to_adjacency(edge_index, num_nodes):\n",
    "    A = torch.zeros(num_nodes, num_nodes)\n",
    "    A[edge_index[0], edge_index[1]] = 1\n",
    "    return A\n",
    "\n",
    "def compute_degree_matrix(A):\n",
    "    degree = A.sum(dim=1)\n",
    "    D = torch.diag(degree)\n",
    "    return D\n",
    "\n",
    "def compute_laplacian(A):\n",
    "    D = compute_degree_matrix(A)\n",
    "    L = D - A\n",
    "    return L\n",
    "\n",
    "# Compute matrices\n",
    "A = edge_index_to_adjacency(graph_data.edge_index, graph_data.num_nodes)\n",
    "D = compute_degree_matrix(A)\n",
    "L = compute_laplacian(A)\n",
    "\n",
    "print(\"Adjacency Matrix A:\")\n",
    "print(A.int())\n",
    "print(\"\\nDegree Matrix D:\")\n",
    "print(D.int())\n",
    "print(\"\\nLaplacian Matrix L = D - A:\")\n",
    "print(L.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Normalized Adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_adjacency(edge_index, num_nodes, add_self_loops=True):\n",
    "    A = edge_index_to_adjacency(edge_index, num_nodes)\n",
    "    if add_self_loops:\n",
    "        A = A + torch.eye(num_nodes)\n",
    "    D = compute_degree_matrix(A)\n",
    "    D_inv_sqrt = torch.diag(1.0 / torch.sqrt(D.diag()))\n",
    "    A_norm = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return A_norm\n",
    "\n",
    "A_norm = compute_normalized_adjacency(graph_data.edge_index, graph_data.num_nodes)\n",
    "print(\"Normalized Adjacency Matrix:\")\n",
    "print(A_norm)\n",
    "print(\"\\nRow sums:\")\n",
    "print(A_norm.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing GCN Layer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayerManual(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GCNLayerManual, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        num_nodes = x.size(0)\n",
    "        A_norm = compute_normalized_adjacency(edge_index, num_nodes, add_self_loops=True)\n",
    "        A_norm = A_norm.to(x.device)\n",
    "        out = A_norm @ x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "\n",
    "# Test the layer\n",
    "gcn_layer = GCNLayerManual(n_features, 8, bias=True)\n",
    "output = gcn_layer(x, graph_data.edge_index)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete GCN Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNManual(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_layers=2, dropout=0.5):\n",
    "        super(GCNManual, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(GCNLayerManual(in_features, hidden_features))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNLayerManual(hidden_features, hidden_features))\n",
    "        self.layers.append(GCNLayerManual(hidden_features, out_features))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        return x\n",
    "\n",
    "# Test the model\n",
    "model = GCNManual(n_features, 32, 4, num_layers=2, dropout=0.5)\n",
    "output = model(x, graph_data.edge_index)\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using PyTorch Geometric GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Geometric Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_layers=2, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(GCNConv(in_features, hidden_features))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_features, hidden_features))\n",
    "        self.layers.append(GCNConv(hidden_features, out_features))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        return x\n",
    "\n",
    "# Test the model\n",
    "model = GCN(n_features, 32, 4, num_layers=2, dropout=0.5)\n",
    "output = model(x, graph_data.edge_index)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Node Classification on Real Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {data.num_node_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Train/Val/Test: {data.train_mask.sum()}/{data.val_mask.sum()}/{data.test_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, criterion, mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[mask], data.y[mask])\n",
    "    pred = out[mask].argmax(dim=1)\n",
    "    acc = (pred == data.y[mask]).sum().item() / mask.sum().item()\n",
    "    return loss.item(), acc\n",
    "\n",
    "print(\"Training and evaluation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = GCN(\n",
    "    in_features=data.num_node_features,\n",
    "    hidden_features=64,\n",
    "    out_features=dataset.num_classes,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training\n",
    "epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_val_acc = 0\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Training GCN for {epochs} epochs...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, data, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, data, criterion, data.val_mask)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        best_state = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        model.load_state_dict(best_state)\n",
    "        break\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = evaluate(model, data, criterion, data.test_mask)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Final Results on Test Set:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(val_accs, label='Val Accuracy', linewidth=2, color='green')\n",
    "axes[1].axhline(y=test_acc, color='red', linestyle='--', linewidth=2, label=f'Test Acc: {test_acc:.4f}')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Validation Accuracy Over Training', fontsize=14)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualization of Learned Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hidden Layer Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, data, layer_idx=-2):\n",
    "    model.eval()\n",
    "    x = data.x\n",
    "    edge_index = data.edge_index\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i < len(model.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "            if i == layer_idx:\n",
    "                return x\n",
    "    return x\n",
    "\n",
    "embeddings_hidden = get_embeddings(model, data, layer_idx=0)\n",
    "print(f\"Hidden embeddings shape: {embeddings_hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing t-SNE reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings_hidden.cpu().numpy())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_tsne[:, 0],\n",
    "    embeddings_tsne[:, 1],\n",
    "    c=data.y.cpu().numpy(),\n",
    "    cmap='tab10',\n",
    "    s=50,\n",
    "    alpha=0.7,\n",
    "    edgecolors='k',\n",
    "    linewidth=0.5\n",
    ")\nplt.title('t-SNE of Hidden Layer Embeddings (Colored by Class)', fontsize=14)\nplt.xlabel('t-SNE 1', fontsize=12)\nplt.ylabel('t-SNE 2', fontsize=12)\nplt.colorbar(scatter, label='Class')\nplt.grid(alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "embeddings_pca = pca.fit_transform(embeddings_hidden.cpu().numpy())\n",
    "\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_pca[:, 0],\n",
    "    embeddings_pca[:, 1],\n",
    "    c=data.y.cpu().numpy(),\n",
    "    cmap='tab10',\n",
    "    s=50,\n",
    "    alpha=0.7,\n",
    "    edgecolors='k',\n",
    "    linewidth=0.5\n",
    ")\nplt.title('PCA of Hidden Layer Embeddings', fontsize=14)\nplt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%})\", fontsize=12)\nplt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%})\", fontsize=12)\nplt.colorbar(scatter, label='Class')\nplt.grid(alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning: Number of Layers\")\nprint(\"=\"*50)\n",
    "\n",
    "results_layers = {}\n",
    "\n",
    "for num_layers in [1, 2, 3, 4]:\n",
    "    print(f\"\\nTraining GCN with {num_layers} layers...\")\n",
    "    \n",
    "    model = GCN(\n",
    "        in_features=data.num_node_features,\n",
    "        hidden_features=64,\n",
    "        out_features=dataset.num_classes,\n",
    "        num_layers=num_layers,\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        train_loss = train(model, data, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, data, criterion, data.val_mask)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_loss, test_acc = evaluate(model, data, criterion, data.test_mask)\n",
    "    \n",
    "    results_layers[num_layers] = {'val_acc': best_val_acc, 'test_acc': test_acc}\n",
    "    print(f\"  Val Acc: {best_val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "for num_layers, results in results_layers.items():\n",
    "    print(f\"  {num_layers} layers: Test Acc = {results['test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTuning: Dropout Rate\")\nprint(\"=\"*50)\n",
    "\n",
    "results_dropout = {}\n",
    "\n",
    "for dropout_rate in [0.0, 0.2, 0.5, 0.7]:\n",
    "    print(f\"\\nTraining with dropout={dropout_rate}...\")\n",
    "    \n",
    "    model = GCN(\n",
    "        in_features=data.num_node_features,\n",
    "        hidden_features=64,\n",
    "        out_features=dataset.num_classes,\n",
    "        num_layers=2,\n",
    "        dropout=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        train_loss = train(model, data, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, data, criterion, data.val_mask)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_loss, test_acc = evaluate(model, data, criterion, data.test_mask)\n",
    "    \n",
    "    results_dropout[dropout_rate] = test_acc\n",
    "    print(f\"  Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "for dropout_rate, test_acc in results_dropout.items():\n",
    "    print(f\"  dropout={dropout_rate}: Test Acc = {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Number of layers\n",
    "layers = list(results_layers.keys())\n",
    "test_accs_layers = [results_layers[l]['test_acc'] for l in layers]\n",
    "axes[0].plot(layers, test_accs_layers, marker='o', linewidth=2, markersize=10)\n",
    "axes[0].set_xlabel('Number of Layers', fontsize=12)\n",
    "axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[0].set_title('Effect of Network Depth', fontsize=14)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(layers)\n",
    "\n",
    "# Dropout rate\n",
    "dropouts = list(results_dropout.keys())\n",
    "test_accs_dropout = [results_dropout[d] for d in dropouts]\n",
    "axes[1].plot(dropouts, test_accs_dropout, marker='s', linewidth=2, markersize=10)\n",
    "axes[1].set_xlabel('Dropout Rate', fontsize=12)\n",
    "axes[1].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[1].set_title('Effect of Dropout', fontsize=14)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these to deepen your understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Add Residual Connections\n",
    "\n",
    "Modify GCN to include skip connections: h_new = h_old + f(h_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement GCN with residual connections\n",
    "print(\"Exercise 1: Add residual connections to prevent over-smoothing\")\n",
    "print(\"Hint: When dimensions match, add skip connection in forward()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Try Other Datasets\n",
    "\n",
    "Test on Citeseer and Pubmed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and train on Citeseer\n",
    "print(\"Exercise 2: Try Citeseer dataset\")\n",
    "print(\"from torch_geometric.datasets import Planetoid\")\n",
    "print(\"dataset = Planetoid(root='/tmp/Citeseer', name='Citeseer')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Batch Normalization\n",
    "\n",
    "Add batch normalization to layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add nn.BatchNorm1d to GCN layers\n",
    "print(\"Exercise 3: Add batch normalization\")\n",
    "print(\"Use nn.BatchNorm1d between layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We covered:\n",
    "1. GCN implementation from scratch and using PyTorch Geometric\n",
    "2. Semi-supervised node classification\n",
    "3. Visualization of learned embeddings\n",
    "4. Hyperparameter optimization\n",
    "5. Architecture experiments\n",
    "\n",
    "Key insights:\n",
    "- GCNs aggregate information from neighbors via normalized adjacency\n",
    "- Semi-supervised learning leverages graph structure\n",
    "- 2-4 layers typically works best (over-smoothing)\n",
    "- Proper regularization is crucial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
