{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Advanced GNN Architectures\n",
    "## GraphSAGE and Graph Isomorphism Networks (GIN)\n",
    "\n",
    "In this notebook, we'll implement and compare two advanced GNN architectures:\n",
    "1. **GraphSAGE**: For inductive learning with neighborhood sampling\n",
    "2. **GIN**: For maximum expressiveness through injective aggregation\n",
    "\n",
    "We'll explore sampling strategies, inductive learning, and expressiveness analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Set, Optional\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "from torch_geometric.data import Data, DataLoader, NeighborSampler\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding GraphSAGE\n",
    "\n",
    "### 1.1 Neighborhood Sampling\n",
    "\n",
    "Let's start by implementing neighborhood sampling mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborhoodSampler:\n",
    "    \"\"\"Base class for neighborhood sampling strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_dict: Dict[int, List[int]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            graph_dict: Dictionary mapping node -> list of neighbors\n",
    "        \"\"\"\n",
    "        self.graph_dict = graph_dict\n",
    "        self.node_degrees = {v: len(neighbors) for v, neighbors in graph_dict.items()}\n",
    "    \n",
    "    def sample_neighbors(self, node: int, sample_size: int) -> List[int]:\n",
    "        \"\"\"Sample neighbors of a node. To be implemented by subclasses.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class UniformSampler(NeighborhoodSampler):\n",
    "    \"\"\"Uniform random sampling of neighbors\"\"\"\n",
    "    \n",
    "    def sample_neighbors(self, node: int, sample_size: int) -> List[int]:\n",
    "        neighbors = self.graph_dict.get(node, [])\n",
    "        if len(neighbors) == 0:\n",
    "            return []\n",
    "        sample_size = min(sample_size, len(neighbors))\n",
    "        return list(np.random.choice(neighbors, size=sample_size, replace=False))\n",
    "\nclass ImportanceSampler(NeighborhoodSampler):\n",
    "    \"\"\"Sample neighbors based on importance scores (e.g., node degree)\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_dict: Dict[int, List[int]], importance_fn=None):\n",
    "        super().__init__(graph_dict)\n",
    "        if importance_fn is None:\n",
    "            # Default: degree-based importance\n",
    "            importance_fn = lambda node: self.node_degrees.get(node, 1)\n",
    "        self.importance_fn = importance_fn\n",
    "    \n",
    "    def sample_neighbors(self, node: int, sample_size: int) -> List[int]:\n",
    "        neighbors = self.graph_dict.get(node, [])\n",
    "        if len(neighbors) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Compute importance scores\n",
    "        scores = np.array([self.importance_fn(n) for n in neighbors])\n",
    "        scores = scores / scores.sum()  # Normalize to probabilities\n",
    "        \n",
    "        sample_size = min(sample_size, len(neighbors))\n",
    "        return list(np.random.choice(neighbors, size=sample_size, replace=False, p=scores))\n",
    "\nclass AdaptiveSampler(NeighborhoodSampler):\n",
    "    \"\"\"Adaptive sampling with variance reduction\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_dict: Dict[int, List[int]], importance_scores: Dict[int, float]):\n",
    "        super().__init__(graph_dict)\n",
    "        self.importance_scores = importance_scores\n",
    "    \n",
    "    def sample_neighbors(self, node: int, sample_size: int) -> List[int]:\n",
    "        neighbors = self.graph_dict.get(node, [])\n",
    "        if len(neighbors) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Use learned importance scores\n",
    "        scores = np.array([self.importance_scores.get(n, 1.0) for n in neighbors])\n",
    "        scores = np.maximum(scores, 0.01)  # Avoid zero probabilities\n",
    "        scores = scores / scores.sum()\n",
    "        \n",
    "        sample_size = min(sample_size, len(neighbors))\n",
    "        return list(np.random.choice(neighbors, size=sample_size, replace=False, p=scores))\n\nprint(\"Neighborhood Sampler classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GraphSAGE Aggregators\n",
    "\n",
    "Implement different aggregation functions for GraphSAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEAggregator(nn.Module):\n",
    "    \"\"\"Base class for GraphSAGE aggregators\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, node_features: torch.Tensor, neighbor_features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Aggregate neighbor features and combine with node features.\n",
    "        \n",
    "        Args:\n",
    "            node_features: Features of central node(s), shape (batch_size, input_dim)\n",
    "            neighbor_features: Features of neighbors, list of (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            aggregated: Combined features, shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n\nclass MeanAggregator(GraphSAGEAggregator):\n",
    "    \"\"\"Mean aggregator: average neighbor embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int, bias: bool = True):\n",
    "        super().__init__(input_dim, output_dim, bias)\n",
    "        self.linear = nn.Linear(input_dim * 2, output_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, node_features: torch.Tensor, neighbor_features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        if len(neighbor_features) == 0:\n",
    "            # No neighbors, use node features only\n",
    "            neighbor_mean = torch.zeros_like(node_features)\n",
    "        else:\n",
    "            neighbor_mean = torch.mean(torch.stack(neighbor_features), dim=0)\n",
    "        \n",
    "        combined = torch.cat([node_features, neighbor_mean], dim=1)\n",
    "        return F.relu(self.linear(combined))\n\nclass PoolingAggregator(GraphSAGEAggregator):\n",
    "    \"\"\"Pooling aggregator: max pooling over neighbor features\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int, bias: bool = True):\n",
    "        super().__init__(input_dim, output_dim, bias)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, input_dim)\n",
    "        )\n",
    "        self.linear = nn.Linear(input_dim * 2, output_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, node_features: torch.Tensor, neighbor_features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        if len(neighbor_features) == 0:\n",
    "            neighbor_pool = torch.zeros_like(node_features)\n",
    "        else:\n",
    "            # Apply MLP to neighbors and max pool\n",
    "            neighbor_mlps = torch.stack([self.mlp(nf) for nf in neighbor_features])\n",
    "            neighbor_pool, _ = torch.max(neighbor_mlps, dim=0)\n",
    "        \n",
    "        combined = torch.cat([node_features, neighbor_pool], dim=1)\n",
    "        return F.relu(self.linear(combined))\n\nclass LSTMAggregator(GraphSAGEAggregator):\n",
    "    \"\"\"LSTM aggregator: sequential aggregation of neighbors\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int, bias: bool = True):\n",
    "        super().__init__(input_dim, output_dim, bias)\n",
    "        self.lstm = nn.LSTM(input_dim, input_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(input_dim * 2, output_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, node_features: torch.Tensor, neighbor_features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        if len(neighbor_features) == 0:\n",
    "            neighbor_lstm = torch.zeros_like(node_features)\n",
    "        else:\n",
    "            # Stack neighbors into sequence\n",
    "            neighbor_seq = torch.stack(neighbor_features, dim=1)  # (batch, seq_len, dim)\n",
    "            _, (hidden, _) = self.lstm(neighbor_seq)\n",
    "            neighbor_lstm = hidden.squeeze(0)\n",
    "        \n",
    "        combined = torch.cat([node_features, neighbor_lstm], dim=1)\n",
    "        return F.relu(self.linear(combined))\n\nprint(\"Aggregator classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 GraphSAGE Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"GraphSAGE: Inductive Representation Learning on Large Graphs\n",
    "    \n",
    "    Key features:\n",
    "    - Inductive learning: learns to embed unseen nodes\n",
    "    - Mini-batch training: scalable to large graphs\n",
    "    - Multiple aggregators: flexible architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int,\n",
    "                 aggregator_type: str = 'mean', dropout: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            hidden_dims: List of hidden layer dimensions\n",
    "            output_dim: Output dimension\n",
    "            aggregator_type: 'mean', 'pooling', or 'lstm'\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Create aggregators for each layer\n",
    "        self.aggregators = nn.ModuleList()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        \n",
    "        aggregator_class = {\n",
    "            'mean': MeanAggregator,\n",
    "            'pooling': PoolingAggregator,\n",
    "            'lstm': LSTMAggregator\n",
    "        }[aggregator_type]\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            self.aggregators.append(aggregator_class(dims[i], dims[i + 1]))\n",
    "    \n",
    "    def forward(self, node_features: torch.Tensor, neighbors_per_layer: List[List[List[int]]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with multi-hop aggregation.\n",
    "        \n",
    "        Args:\n",
    "            node_features: Node feature embeddings, shape (num_nodes, input_dim)\n",
    "            neighbors_per_layer: neighbors_per_layer[layer][node_idx] = list of neighbor indices\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: Final embeddings, shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = len(neighbors_per_layer[0])\n",
    "        current_features = node_features[:batch_size]\n",
    "        \n",
    "        for layer_idx, aggregator in enumerate(self.aggregators):\n",
    "            neighbor_indices = neighbors_per_layer[layer_idx]\n",
    "            neighbor_features = []\n",
    "            \n",
    "            for node_idx, neighbors in enumerate(neighbor_indices):\n",
    "                if neighbors:\n",
    "                    neighbor_feats = node_features[neighbors]\n",
    "                    neighbor_features.append(neighbor_feats)\n",
    "                else:\n",
    "                    neighbor_features.append(torch.zeros(0, node_features.shape[1], device=node_features.device))\n",
    "            \n",
    "            # Aggregate\n",
    "            aggregated = aggregator(current_features, neighbor_features)\n",
    "            current_features = F.dropout(aggregated, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return current_features\n\nprint(\"GraphSAGE model defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Graph Isomorphism Networks (GIN)\n",
    "\n",
    "### 2.1 Understanding Weisfeiler-Lehman Test\n",
    "\n",
    "Implement the Weisfeiler-Lehman (WL) graph isomorphism test to understand GIN's theoretical foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weisfeiler_lehman_test(graph_dict: Dict[int, List[int]], num_iterations: int = 3) -> Dict:\n",
    "    \"\"\"Perform Weisfeiler-Lehman test on a graph.\n",
    "    \n",
    "    Args:\n",
    "        graph_dict: Adjacency list representation\n",
    "        num_iterations: Number of WL iterations\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with iteration-by-iteration color assignments\n",
    "    \"\"\"\n",
    "    num_nodes = len(graph_dict)\n",
    "    colors = {}\n",
    "    color_sequence = {}\n",
    "    \n",
    "    # Iteration 0: Color by degree\n",
    "    for node in range(num_nodes):\n",
    "        colors[node] = len(graph_dict.get(node, []))\n",
    "    color_sequence[0] = colors.copy()\n",
    "    \n",
    "    # Perform WL iterations\n",
    "    for iteration in range(num_iterations):\n",
    "        new_colors = {}\n",
    "        color_map = {}  # Map unique signatures to new colors\n",
    "        next_color = 0\n",
    "        \n",
    "        for node in range(num_nodes):\n",
    "            # Get current color\n",
    "            current_color = colors[node]\n",
    "            \n",
    "            # Get sorted colors of neighbors\n",
    "            neighbor_colors = sorted([colors[n] for n in graph_dict.get(node, [])])\n",
    "            \n",
    "            # Create signature\n",
    "            signature = (current_color, tuple(neighbor_colors))\n",
    "            \n",
    "            # Map signature to new color\n",
    "            if signature not in color_map:\n",
    "                color_map[signature] = next_color\n",
    "                next_color += 1\n",
    "            \n",
    "            new_colors[node] = color_map[signature]\n",
    "        \n",
    "        colors = new_colors\n",
    "        color_sequence[iteration + 1] = colors.copy()\n",
    "    \n",
    "    return color_sequence\n\nprint(\"Weisfeiler-Lehman test function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GIN Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLayer(nn.Module):\n",
    "    \"\"\"Graph Isomorphism Network (GIN) Layer\n",
    "    \n",
    "    Mathematical formulation:\n",
    "    h_v^{k+1} = MLP^k((1 + eps_k) * h_v^k + sum_{u in N(v)} h_u^k)\n",
    "    \n",
    "    Key property: Injective aggregation preserves injectivity of node functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: Optional[int] = None,\n",
    "                 eps: float = 0.0, learn_eps: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            output_dim: Output feature dimension\n",
    "            hidden_dim: Hidden dimension for MLP (default: output_dim)\n",
    "            eps: Initial epsilon value\n",
    "            learn_eps: Whether to learn epsilon\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learn_eps = learn_eps\n",
    "        \n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = output_dim\n",
    "        \n",
    "        # Register epsilon\n",
    "        if learn_eps:\n",
    "            self.eps = nn.Parameter(torch.tensor(eps, dtype=torch.float32))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.tensor(eps, dtype=torch.float32))\n",
    "        \n",
    "        # MLP: (1+eps)*x + sum(neighbors) -> output\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, node_features: torch.Tensor, neighbor_sum: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            node_features: Features of nodes, shape (num_nodes, input_dim)\n",
    "            neighbor_sum: Sum of neighbor features, shape (num_nodes, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            updated: Updated node features, shape (num_nodes, output_dim)\n",
    "        \"\"\"\n",
    "        # Apply GIN formula: MLP((1+eps)*x + sum(neighbors))\n",
    "        aggregated = (1 + self.eps) * node_features + neighbor_sum\n",
    "        return self.mlp(aggregated)\n\nclass GIN(nn.Module):\n",
    "    \"\"\"Graph Isomorphism Network\n",
    "    \n",
    "    Theoretically grounded architecture with guaranteed expressiveness\n",
    "    matching the Weisfeiler-Lehman graph isomorphism test.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int,\n",
    "                 dropout: float = 0.0, learn_eps: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            hidden_dims: List of hidden layer dimensions\n",
    "            output_dim: Output dimension\n",
    "            dropout: Dropout probability\n",
    "            learn_eps: Whether to learn epsilon in GIN layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.gin_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            self.gin_layers.append(GINLayer(dims[i], dims[i + 1], learn_eps=learn_eps))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features, shape (num_nodes, input_dim)\n",
    "            edge_index: Edge indices, shape (2, num_edges)\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: Final embeddings, shape (num_nodes, output_dim)\n",
    "        \"\"\"\n",
    "        for i, gin_layer in enumerate(self.gin_layers):\n",
    "            # Compute neighbor sum via sparse matrix multiplication\n",
    "            num_nodes = x.shape[0]\n",
    "            \n",
    "            # Create adjacency matrix\n",
    "            adj = torch.sparse.FloatTensor(\n",
    "                edge_index,\n",
    "                torch.ones(edge_index.shape[1], device=edge_index.device),\n",
    "                torch.Size([num_nodes, num_nodes])\n",
    "            ).to(x.device)\n",
    "            \n",
    "            # Sum of neighbor features\n",
    "            neighbor_sum = torch.sparse.mm(adj, x)\n",
    "            \n",
    "            # Apply GIN layer\n",
    "            x = gin_layer(x, neighbor_sum)\n",
    "            \n",
    "            if i < len(self.gin_layers) - 1:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return x\n    \n",
    "    def readout(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Graph-level readout: sum all node embeddings\"\"\"\n",
    "        return torch.sum(x, dim=0, keepdim=True)\n\nprint(\"GIN model defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Practical Experiments\n",
    "\n",
    "### 3.1 Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset for node classification\n",
    "dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Feature dimension: {data.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Train/Val/Test split: {data.train_mask.sum()}/{data.val_mask.sum()}/{data.test_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Neighborhood Sampling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to adjacency list\ndef edge_index_to_adj_list(edge_index: torch.Tensor, num_nodes: int) -> Dict[int, List[int]]:\n",
    "    \"\"\"Convert edge_index to adjacency list\"\"\"\n",
    "    adj_list = defaultdict(list)\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        u, v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "        adj_list[u].append(v)\n",
    "    # Ensure all nodes are in dict\n",
    "    for i in range(num_nodes):\n",
    "        if i not in adj_list:\n",
    "            adj_list[i] = []\n",
    "    return dict(adj_list)\n\nadj_list = edge_index_to_adj_list(data.edge_index, data.num_nodes)\n\n# Compare sampling strategies\nuniform_sampler = UniformSampler(adj_list)\nimportance_sampler = ImportanceSampler(adj_list)\n\nnode_id = 0\nsample_size = 5\n\nprint(f\"Node {node_id} neighbors: {adj_list[node_id][:10]}...\")  # Show first 10\nprint(f\"Degree: {len(adj_list[node_id])}\")\nprint()\n\n# Sample multiple times to analyze variance\nnum_samples = 100\nuniform_samples = []\nimportance_samples = []\n\nfor _ in range(num_samples):\n",
    "    u_sample = set(uniform_sampler.sample_neighbors(node_id, sample_size))\n",
    "    i_sample = set(importance_sampler.sample_neighbors(node_id, sample_size))\n",
    "    uniform_samples.append(u_sample)\n",
    "    importance_samples.append(i_sample)\n",
    "\nprint(f\"Uniform sampler - avg sample size: {np.mean([len(s) for s in uniform_samples]):.2f}\")\nprint(f\"Importance sampler - avg sample size: {np.mean([len(s) for s in importance_samples]):.2f}\")\nprint()\nprint(\"Sample diversity (Jaccard similarity):\")\nprint(f\"Uniform: {np.mean([len(uniform_samples[i] & uniform_samples[j]) / len(uniform_samples[i] | uniform_samples[j]) for i in range(10) for j in range(i+1, 10)]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Weisfeiler-Lehman Test Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small example graphs\ndef create_cycle_graph(n: int) -> Dict[int, List[int]]:\n",
    "    \"\"\"Create a cycle graph\"\"\"\n",
    "    graph = {i: [] for i in range(n)}\n",
    "    for i in range(n):\n",
    "        graph[i] = [(i - 1) % n, (i + 1) % n]\n",
    "    return graph\n\ndef create_complete_bipartite_graph(n: int) -> Dict[int, List[int]]:\n",
    "    \"\"\"Create complete bipartite graph K_{n,n}\"\"\"\n",
    "    graph = {i: [] for i in range(2 * n)}\n",
    "    for i in range(n):\n",
    "        for j in range(n, 2 * n):\n",
    "            graph[i].append(j)\n",
    "            graph[j].append(i)\n",
    "    return graph\n\n# Create example graphs\ncycle_graph = create_cycle_graph(6)\nbipartite_graph = create_complete_bipartite_graph(3)\n\nprint(\"Cycle graph (6 nodes):\")\nprint(cycle_graph)\nprint()\nprint(\"Complete bipartite graph K_{3,3}:\")\nprint(bipartite_graph)\nprint()\n\n# Run WL test\ncycle_wl = weisfeiler_lehman_test(cycle_graph, num_iterations=2)\nbipartite_wl = weisfeiler_lehman_test(bipartite_graph, num_iterations=2)\n\nprint(\"Cycle graph WL color evolution:\")\nfor it, colors in cycle_wl.items():\n",
    "    unique_colors = len(set(colors.values()))\n",
    "    print(f\"  Iteration {it}: {unique_colors} unique colors\")\n",
    "\nprint()\nprint(\"Bipartite graph WL color evolution:\")\nfor it, colors in bipartite_wl.items():\n",
    "    unique_colors = len(set(colors.values()))\n",
    "    print(f\"  Iteration {it}: {unique_colors} unique colors\")\n",
    "\n# Check if graphs are distinguished\ncycle_final = tuple(sorted(cycle_wl[2].values()))\nbipartite_final = tuple(sorted(bipartite_wl[2].values()))\nprint()\nprint(f\"Graphs distinguished by WL test: {cycle_final != bipartite_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 GIN Layer Expressiveness Test\n",
    "\n",
    "Verify that GIN can distinguish graphs based on WL test expressiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_list_to_edge_index(adj_list: Dict[int, List[int]]) -> torch.Tensor:\n",
    "    \"\"\"Convert adjacency list to edge_index tensor\"\"\"\n",
    "    edges = []\n",
    "    for u, neighbors in adj_list.items():\n",
    "        for v in neighbors:\n",
    "            edges.append([u, v])\n",
    "    if edges:\n",
    "        return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        return torch.zeros((2, 0), dtype=torch.long)\n\n# Create small graphs for expressiveness test\ngraph1 = create_cycle_graph(4)\ngraph2 = create_complete_bipartite_graph(2)\n\nedge1 = adjacency_list_to_edge_index(graph1)\nedge2 = adjacency_list_to_edge_index(graph2)\n\nnum_nodes_1 = len(graph1)\nnum_nodes_2 = len(graph2)\n\nprint(f\"Graph 1 (cycle): {num_nodes_1} nodes, {edge1.shape[1]} edges\")\nprint(f\"Graph 2 (bipartite): {num_nodes_2} nodes, {edge2.shape[1]} edges\")\nprint()\n\n# Create GIN model\ngin = GIN(input_dim=1, hidden_dims=[16], output_dim=8)\n\n# Initialize node features (all ones for simplicity)\nfeatures1 = torch.ones(num_nodes_1, 1)\nfeatures2 = torch.ones(num_nodes_2, 1)\n\nprint(\"Testing GIN expressiveness on different graph structures...\")\nprint()\n\n# Forward pass\nwith torch.no_grad():\n",
    "    embeddings1 = gin(features1, edge1)\n",
    "    embeddings2 = gin(features2, edge2)\n",
    "    \n",
    "    # Compute graph-level readouts\n",
    "    readout1 = gin.readout(embeddings1)\n",
    "    readout2 = gin.readout(embeddings2)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = torch.cosine_similarity(readout1, readout2).item()\n",
    "\nprint(f\"Graph 1 readout shape: {readout1.shape}\")\nprint(f\"Graph 2 readout shape: {readout2.shape}\")\nprint(f\"Cosine similarity: {similarity:.4f}\")\nprint(f\"Are graphs sufficiently different (sim < 0.95): {similarity < 0.95}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Training: Inductive vs Transductive Learning\n",
    "\n",
    "Compare inductive (GraphSAGE-like) and transductive (full-batch) learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(nn.Module):\n",
    "    \"\"\"Wrapper for node classification task\"\"\"\n",
    "    \n",
    "    def __init__(self, gnn_model: nn.Module, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.gnn = gnn_model\n",
    "        self.classifier = nn.Linear(gnn_model.output_dim, num_classes)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        embeddings = self.gnn(*args, **kwargs)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n",
    "\ndef train_epoch(model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                labels: torch.Tensor, mask: torch.Tensor) -> float:\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits = model(x, edge_index)\n",
    "    loss = F.cross_entropy(logits[mask], labels[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\ndef evaluate(model: nn.Module, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "             labels: torch.Tensor, mask: torch.Tensor) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, edge_index)\n",
    "        loss = F.cross_entropy(logits[mask], labels[mask])\n",
    "        pred = logits[mask].argmax(dim=1)\n",
    "        acc = accuracy_score(labels[mask].cpu(), pred.cpu())\n",
    "    return loss.item(), acc\n\n# Create models\ngin_model = GIN(input_dim=data.num_features, hidden_dims=[64, 64], output_dim=32, dropout=0.5)\ngin_classifier = NodeClassifier(gin_model, dataset.num_classes)\n\n# Train GIN\nprint(\"Training GIN model...\")\noptimizer = Adam(gin_classifier.parameters(), lr=0.01, weight_decay=5e-4)\n\ntrain_losses = []\nval_accs = []\ntest_accs = []\n\nfor epoch in range(100):\n",
    "    train_loss = train_epoch(gin_classifier, optimizer, data.x, data.edge_index,\n",
    "                             data.y, data.train_mask)\n",
    "    val_loss, val_acc = evaluate(gin_classifier, data.x, data.edge_index,\n",
    "                                 data.y, data.val_mask)\n",
    "    test_loss, test_acc = evaluate(gin_classifier, data.x, data.edge_index,\n",
    "                                   data.y, data.test_mask)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\nprint(f\"\\nFinal GIN Performance:\")\nprint(f\"  Val Accuracy: {val_accs[-1]:.4f}\")\nprint(f\"  Test Accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different aggregators in a toy GraphSAGE model\nprint(\"Comparing GraphSAGE aggregators on small graph...\")\nprint()\n\n# Create small feature matrix\nsmall_x = torch.randn(10, 8)\nedge_index_small = torch.tensor([[0, 1, 2, 3, 4], [1, 2, 3, 4, 5]], dtype=torch.long)\n\n# Define sample neighborhoods\nneighbors_layer0 = [[1, 2], [0, 2, 3], [1, 3], [2, 4], [3, 5], [4], [7], [6, 8], [7, 9], [8]]\nneighbor_indices = [neighbors_layer0]  # Just one layer for demo\n\n# Test each aggregator\nprint(\"Aggregator Comparison:\")\nprint(\"-\" * 60)\n\nfor agg_name in ['mean', 'pooling', 'lstm']:\n",
    "    try:\n",
    "        sage = GraphSAGE(input_dim=8, hidden_dims=[16], output_dim=8, aggregator_type=agg_name)\n",
    "        with torch.no_grad():\n",
    "            output = sage(small_x, [neighbors_layer0])\n",
    "        params = sum(p.numel() for p in sage.parameters())\n",
    "        print(f\"{agg_name.capitalize():10s} | Output shape: {output.shape} | Params: {params}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{agg_name.capitalize():10s} | Error: {str(e)[:40]}\")\n",
    "\nprint()\nprint(\"Note: Different aggregators have different computational characteristics:\")\nprint(\"  - Mean: Fastest, simplest\")\nprint(\"  - Pooling: Captures non-linear neighbor interactions\")\nprint(\"  - LSTM: Sequential processing, order-dependent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Sampling Impact on Performance\n",
    "\n",
    "Analyze how different sampling strategies affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sampling impact\nprint(\"Sampling Strategy Analysis\")\nprint(\"=\" * 60)\nprint()\n",
    "\n# Sample from different sample sizes\nsample_sizes = [5, 10, 25, 50]\nresults = {'sample_size': [], 'uniform_coverage': [], 'importance_coverage': []}\n\nfor sample_size in sample_sizes:\n",
    "    # Test on high-degree nodes\n",
    "    high_degree_nodes = sorted([(len(adj_list[n]), n) for n in range(data.num_nodes)],\n",
    "                               reverse=True)[:10]\n",
    "    high_degree_nodes = [n for _, n in high_degree_nodes]\n",
    "    \n",
    "    uniform_coverage = []\n",
    "    importance_coverage = []\n",
    "    \n",
    "    for node in high_degree_nodes:\n",
    "        degree = len(adj_list[node])\n",
    "        if degree > 0:\n",
    "            u_sample = uniform_sampler.sample_neighbors(node, sample_size)\n",
    "            i_sample = importance_sampler.sample_neighbors(node, sample_size)\n",
    "            uniform_coverage.append(len(u_sample) / degree)\n",
    "            importance_coverage.append(len(i_sample) / degree)\n",
    "    \n",
    "    results['sample_size'].append(sample_size)\n",
    "    results['uniform_coverage'].append(np.mean(uniform_coverage))\n",
    "    results['importance_coverage'].append(np.mean(importance_coverage))\n",
    "\nprint(f\"{'Sample Size':<15} {'Uniform Cov':<15} {'Importance Cov':<15}\")\nprint(\"-\" * 45)\nfor i, size in enumerate(sample_sizes):\n",
    "    print(f\"{size:<15} {results['uniform_coverage'][i]:<15.4f} {results['importance_coverage'][i]:<15.4f}\")\n",
    "\nprint()\nprint(\"Observations:\")\nprint(\"  - Coverage increases with sample size (as expected)\")\nprint(\"  - Importance sampling prioritizes high-degree neighbors\")\nprint(\"  - Trade-off between bias and variance in sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Training Dynamics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training dynamics\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n# Plot 1: Training loss\naxes[0].plot(train_losses, label='Training Loss', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('GIN: Training Loss Over Time')\naxes[0].grid(True, alpha=0.3)\naxes[0].legend()\n",
    "\n# Plot 2: Validation and test accuracy\naxes[1].plot(val_accs, label='Validation Accuracy', linewidth=2)\naxes[1].plot(test_accs, label='Test Accuracy', linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('GIN: Node Classification Accuracy')\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\naxes[1].set_ylim([0, 1])\n",
    "\nplt.tight_layout()\nplt.savefig('training_dynamics.png', dpi=150, bbox_inches='tight')\nplt.show()\n",
    "\nprint(f\"Training dynamics visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Model Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"Count trainable parameters in model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\nprint(\"Model Complexity Analysis\")\nprint(\"=\" * 60)\nprint()\n",
    "\n# Create different model sizes\nconfigs = [\n",
    "    {'name': 'Small', 'hidden_dims': [32]},\n",
    "    {'name': 'Medium', 'hidden_dims': [64, 64]},\n",
    "    {'name': 'Large', 'hidden_dims': [128, 128, 128]},\n",
    "]\n",
    "\nprint(f\"{'Model':<12} {'Architecture':<30} {'Parameters':<15}\")\nprint(\"-\" * 57)\n",
    "\nfor config in configs:\n",
    "    gin = GIN(input_dim=data.num_features, hidden_dims=config['hidden_dims'],\n",
    "              output_dim=32)\n",
    "    classifier = NodeClassifier(gin, dataset.num_classes)\n",
    "    params = count_parameters(classifier)\n",
    "    arch_str = f\"[{data.num_features}\"] + [str(h) for h in config['hidden_dims']] + ['32', str(dataset.num_classes)]\n",
    "    arch_str = '-'.join(arch_str)\n",
    "    print(f\"{config['name']:<12} {arch_str:<30} {params:<15}\")\n",
    "\nprint()\nprint(\"Observation: Larger models have more capacity but risk overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Exercises\n",
    "\n",
    "### Exercise 1: Implement Custom Aggregator\n",
    "\n",
    "Create a custom aggregator for GraphSAGE that combines mean and attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1\n",
    "# Implement an AttentionAggregator for GraphSAGE that:\n",
    "# 1. Computes attention weights over neighbors\n",
    "# 2. Aggregates neighbors using attention-weighted sum\n",
    "# 3. Combines with node features\n",
    "\n",
    "class AttentionAggregator(GraphSAGEAggregator):\n",
    "    \"\"\"Custom attention-based aggregator\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int, bias: bool = True):\n",
    "        super().__init__(input_dim, output_dim, bias)\n",
    "        # TODO: Implement attention mechanism\n",
    "        # Hints:\n",
    "        # - Create attention parameters\n",
    "        # - Use softmax for normalized weights\n",
    "        # - Combine attention-weighted neighbors with node features\n",
    "        pass\n",
    "    \n",
    "    def forward(self, node_features: torch.Tensor, neighbor_features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # TODO: Implement forward pass\n",
    "        pass\n",
    "\n# Test your implementation\nprint(\"Exercise 1: Implement custom aggregator\")\nprint(\"See TODO above for implementation details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Analyze GIN Expressiveness\n",
    "\n",
    "Create non-isomorphic graphs that WL test cannot distinguish and verify GIN's limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2\n",
    "# Find or create pairs of non-isomorphic graphs that:\n",
    "# 1. Have same WL coloring sequence\n",
    "# 2. Are structurally different\n",
    "# 3. Cannot be distinguished by GIN\n",
    "\n",
    "# Classic example: McKay graphs (require careful construction)\n",
    "# Simpler alternative: Regular graphs with same degree distribution\n",
    "\n",
    "print(\"Exercise 2: GIN Expressiveness Analysis\")\nprint(\"TODO: Create pairs of graphs that GIN cannot distinguish\")\nprint()\nprint(\"Hints:\")\nprint(\"  1. Start with regular graphs (all nodes have same degree)\")\nprint(\"  2. Create different graph structures with same degree\")\nprint(\"  3. Run WL test to verify they have same coloring\")\nprint(\"  4. Pass to GIN and verify similar embeddings\")\nprint()\nprint(\"Example: 6-node cycle vs 6-node disjoint edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Inductive Learning\n",
    "\n",
    "Modify GraphSAGE to perform true inductive learning on unseen nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 3\n",
    "# Implement inductive node classification where:\n",
    "# 1. Train on subset of graph\n",
    "# 2. Evaluate on completely unseen nodes\n",
    "# 3. Use only aggregation functions (no node embeddings)\n",
    "\n",
    "def inductive_split(num_nodes: int, train_ratio: float = 0.6,\n",
    "                    test_ratio: float = 0.2) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Split graph into train/val/test where test nodes are completely unseen during training.\n",
    "    \n",
    "    Returns:\n",
    "        train_mask, val_mask, test_mask\n",
    "    \"\"\"\n",
    "    # TODO: Implement split\n",
    "    pass\n",
    "\nprint(\"Exercise 3: Inductive Learning\")\nprint(\"TODO: Implement inductive split and train GraphSAGE on unseen nodes\")\nprint()\nprint(\"Steps:\")\nprint(\"  1. Create inductive split (train/val/test are disjoint)\")\nprint(\"  2. Only use training node neighborhoods for sampling\")\nprint(\"  3. Evaluate on test nodes using only aggregation functions\")\nprint(\"  4. Compare inductive vs transductive performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Hyperparameter Tuning\n",
    "\n",
    "Find optimal hyperparameters for both architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 4\n",
    "# Perform hyperparameter search for GIN and GraphSAGE:\n",
    "# - Learning rate: [0.001, 0.01, 0.1]\n",
    "# - Dropout: [0.0, 0.3, 0.5]\n",
    "# - Hidden dims: [[32], [64], [64, 64], [128, 128]]\n",
    "# - Layer depth: [1, 2, 3, 4]\n",
    "\nimport itertools\n\nhyperparams = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'dropout': [0.0, 0.3],\n",
    "    'hidden_dim': [32, 64],\n",
    "}\n",
    "\nprint(\"Exercise 4: Hyperparameter Tuning\")\nprint(f\"Total configurations: {np.prod([len(v) for v in hyperparams.values()])}\")\nprint()\nprint(\"TODO: Implement grid search over hyperparameters\")\nprint(\"Hints:\")\nprint(\"  1. Use early stopping to avoid overfitting\")\nprint(\"  2. Track val accuracy for hyperparameter selection\")\nprint(\"  3. Store results in dictionary for comparison\")\nprint(\"  4. Visualize results (accuracy vs hyperparameters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Extend to Link Prediction\n",
    "\n",
    "Adapt the architectures for link prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 5\n",
    "# Implement link prediction using GIN:\n",
    "# 1. Use node embeddings for link scoring\n",
    "# 2. Implement edge prediction layer\n",
    "# 3. Create negative sampling for training\n",
    "# 4. Evaluate using AUC metric\n",
    "\nclass LinkPredictor(nn.Module):\n",
    "    \"\"\"Link prediction head\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        # TODO: Implement link scoring function\n",
    "        # Options:\n",
    "        # - Inner product: z_i^T z_j\n",
    "        # - MLP: MLP([z_i || z_j])\n",
    "        # - Bilinear: z_i^T W z_j\n",
    "        pass\n",
    "    \n",
    "    def forward(self, z: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Score edges\n",
    "        pass\n",
    "\nprint(\"Exercise 5: Link Prediction\")\nprint(\"TODO: Implement link prediction using graph embeddings\")\nprint()\nprint(\"Steps:\")\nprint(\"  1. Create LinkPredictor head for edge scoring\")\nprint(\"  2. Implement negative sampling for training\")\nprint(\"  3. Train link predictor end-to-end\")\nprint(\"  4. Evaluate using ROC-AUC on validation/test edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we explored two advanced GNN architectures:\n",
    "\n",
    "### GraphSAGE (Inductive Learning)\n",
    "- **Key innovation**: Neighborhood sampling enables inductive learning\n",
    "- **Aggregators**: Mean, LSTM, pooling for flexible information aggregation\n",
    "- **Scalability**: Mini-batch training for large graphs\n",
    "- **Use case**: Production systems, evolving graphs, unseen nodes\n",
    "\n",
    "### GIN (Expressive Power)\n",
    "- **Key innovation**: Injective aggregation matches WL test expressiveness\n",
    "- **Theory**: Grounded in graph isomorphism theory\n",
    "- **Simplicity**: Clean mathematical formulation\n",
    "- **Use case**: Graph-level tasks, when theoretical guarantees matter\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Sampling strategies** dramatically affect scalability\n",
    "2. **Inductive learning** enables generalization to unseen nodes\n",
    "3. **Expressiveness** bounds explain GNN limitations\n",
    "4. **Trade-offs** between theory, empirical performance, and scalability\n",
    "5. **Architecture choice** depends on specific problem requirements\n",
    "\n",
    "### Next Steps\n",
    "- Implement custom aggregators\n",
    "- Explore more sampling strategies (importance, subgraph)\n",
    "- Study higher-order GNN expressiveness\n",
    "- Apply to real-world problems (recommendation systems, knowledge graphs)\n",
    "- Investigate temporal and heterogeneous graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
