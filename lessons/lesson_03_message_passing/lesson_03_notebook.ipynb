{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Message Passing & GNN Foundations\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lesson explores the core concept behind Graph Neural Networks: **message passing**. We'll understand how information propagates through a graph, implement message passing from scratch, and visualize each step of the process.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand the message passing mechanism\n",
    "2. Implement message passing from scratch\n",
    "3. Explore different aggregation functions\n",
    "4. Understand receptive fields in GNNs\n",
    "5. Recognize the over-smoothing problem\n",
    "6. Build a simple GNN for node classification\n",
    "\n",
    "### Key Concepts\n",
    "- **Message Passing**: Core mechanism where nodes exchange information with neighbors\n",
    "- **Aggregation**: Combining messages from multiple neighbors (sum, mean, max, etc.)\n",
    "- **Receptive Field**: The set of nodes that can influence a target node's representation\n",
    "- **Over-smoothing**: Problem where nodes become indistinguishable after many layers\n",
    "- **Multi-layer GNN**: Stacking multiple message passing layers for deeper models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import defaultdict, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('All imports successful!')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Message Passing\n",
    "\n",
    "### Concept\n",
    "\n",
    "Message passing is the fundamental operation in Graph Neural Networks. The idea is simple:\n",
    "\n",
    "1. **Message Computation**: Each node prepares a message based on its features\n",
    "2. **Message Passing**: Messages are sent along edges to neighbors\n",
    "3. **Aggregation**: Each node collects messages from all its neighbors\n",
    "4. **Update**: Node updates its representation based on aggregated messages\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For a given layer $l$:\n",
    "\n",
    "$$m_i^{(l)} = \\\\text{AGGREGATE}\\\\left(\\\\{h_j^{(l)} : j \\\\in \\\\mathcal{N}(i)\\\\}\\\\right)$$\n",
    "\n",
    "$$h_i^{(l+1)} = \\\\sigma\\\\left(W \\\\cdot [h_i^{(l)} || m_i^{(l)}]\\\\right)$$\n",
    "\n",
    "where:\n",
    "- $h_i^{(l)}$ is the hidden state of node $i$ at layer $l$\n",
    "- $\\\\mathcal{N}(i)$ is the set of neighbors of node $i$\n",
    "- $m_i^{(l)}$ is the aggregated message\n",
    "- $W$ is a learnable weight matrix\n",
    "- $\\\\sigma$ is an activation function\n",
    "- $||$ denotes concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Message Passing from Scratch\n",
    "\n",
    "Let's implement message passing step-by-step without using any GNN libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMessagePassing:\n",
    "    \"\"\"\n",
    "    A simple message passing implementation from scratch.\n",
    "    \n",
    "    This class demonstrates the basic message passing mechanism:\n",
    "    1. Each node sends its features to its neighbors\n",
    "    2. Each node aggregates messages from neighbors\n",
    "    3. Each node updates its representation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, feature_dim, aggregation='mean'):\n",
    "        \"\"\"\n",
    "        Initialize the message passing layer.\n",
    "        \n",
    "        Args:\n",
    "            num_nodes: Number of nodes in the graph\n",
    "            feature_dim: Dimension of node features\n",
    "            aggregation: Type of aggregation ('mean', 'sum', 'max')\n",
    "        \"\"\"\n",
    "        self.num_nodes = num_nodes\n",
    "        self.feature_dim = feature_dim\n",
    "        self.aggregation = aggregation\n",
    "        \n",
    "        # Initialize node features\n",
    "        self.features = np.random.randn(num_nodes, feature_dim)\n",
    "        \n",
    "        # Initialize learnable weight matrix\n",
    "        self.W = np.random.randn(feature_dim, feature_dim) * 0.1\n",
    "        \n",
    "        # Store adjacency list\n",
    "        self.adj_list = defaultdict(list)\n",
    "        \n",
    "        # Store history for visualization\n",
    "        self.history = []\n",
    "    \n",
    "    def add_edge(self, u, v):\n",
    "        \"\"\"Add an undirected edge between nodes u and v.\"\"\"\n",
    "        self.adj_list[u].append(v)\n",
    "        self.adj_list[v].append(u)\n",
    "    \n",
    "    def message_fn(self, node_features):\n",
    "        \"\"\"\n",
    "        Message function: prepares features as messages.\n",
    "        \"\"\"\n",
    "        return node_features\n",
    "    \n",
    "    def aggregate_fn(self, messages):\n",
    "        \"\"\"\n",
    "        Aggregation function: combines messages from neighbors.\n",
    "        \"\"\"\n",
    "        if len(messages) == 0:\n",
    "            return np.zeros(self.feature_dim)\n",
    "        \n",
    "        messages = np.array(messages)\n",
    "        \n",
    "        if self.aggregation == 'mean':\n",
    "            return np.mean(messages, axis=0)\n",
    "        elif self.aggregation == 'sum':\n",
    "            return np.sum(messages, axis=0)\n",
    "        elif self.aggregation == 'max':\n",
    "            return np.max(messages, axis=0)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown aggregation: {self.aggregation}')\n",
    "    \n",
    "    def update_fn(self, node_feature, aggregated_message):\n",
    "        \"\"\"\n",
    "        Update function: combines node's features with aggregated message.\n",
    "        \"\"\"\n",
    "        combined = np.concatenate([node_feature, aggregated_message])\n",
    "        updated = node_feature * 0.5 + aggregated_message * 0.5\n",
    "        updated = np.tanh(updated)\n",
    "        return updated\n",
    "    \n",
    "    def forward(self, num_layers=1, return_history=False):\n",
    "        \"\"\"\n",
    "        Perform message passing for specified number of layers.\n",
    "        \"\"\"\n",
    "        if return_history:\n",
    "            history = [self.features.copy()]\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            new_features = np.zeros_like(self.features)\n",
    "            \n",
    "            for node_id in range(self.num_nodes):\n",
    "                neighbor_ids = self.adj_list[node_id]\n",
    "                neighbor_features = [self.features[nid] for nid in neighbor_ids]\n",
    "                \n",
    "                messages = [self.message_fn(f) for f in neighbor_features]\n",
    "                aggregated = self.aggregate_fn(messages)\n",
    "                new_features[node_id] = self.update_fn(self.features[node_id], aggregated)\n",
    "            \n",
    "            self.features = new_features\n",
    "            \n",
    "            if return_history:\n",
    "                history.append(self.features.copy())\n",
    "        \n",
    "        if return_history:\n",
    "            return self.features, history\n",
    "        return self.features\n",
    "\n",
    "\n",
    "print('SimpleMessagePassing class created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualization of Message Passing Steps\n",
    "\n",
    "Let's create a simple graph and visualize how information flows through message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "    (1, 5), (5, 6), (3, 7), (4, 8)\n",
    "])\n",
    "\n",
    "# Initialize message passing\n",
    "mp = SimpleMessagePassing(num_nodes=9, feature_dim=3, aggregation='mean')\n",
    "\n",
    "# Add edges to message passing\n",
    "for edge in G.edges():\n",
    "    mp.add_edge(edge[0], edge[1])\n",
    "\n",
    "# Perform message passing and collect history\n",
    "_, history = mp.forward(num_layers=3, return_history=True)\n",
    "\n",
    "print(f'Graph has {len(G.nodes())} nodes and {len(G.edges())} edges')\n",
    "print(f'Collected history for {len(history)} layers')\n",
    "print(f'Feature dimension: {mp.feature_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_message_passing_steps(G, history):\n",
    "    \"\"\"\n",
    "    Visualize how node features evolve through message passing layers.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(history), figsize=(15, 4))\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42, k=2)\n",
    "    \n",
    "    for layer, (ax, features) in enumerate(zip(axes, history)):\n",
    "        node_colors = features[:, 0]\n",
    "        node_colors_norm = (node_colors - node_colors.min()) / (node_colors.max() - node_colors.min() + 1e-8)\n",
    "        \n",
    "        nx.draw_networkx_edges(G, pos, ax=ax, width=1.5, alpha=0.6)\n",
    "        nodes = nx.draw_networkx_nodes(\n",
    "            G, pos, ax=ax,\n",
    "            node_color=node_colors_norm,\n",
    "            node_size=800,\n",
    "            cmap='viridis',\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "        nx.draw_networkx_labels(G, pos, ax=ax, font_size=10, font_weight='bold')\n",
    "        \n",
    "        ax.set_title(f'Layer {layer}', fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.colorbar(nodes, ax=axes[-1], label='Feature Value')\n",
    "    plt.suptitle('Message Passing Evolution Through Layers', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_message_passing_steps(G, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparing Aggregation Functions\n",
    "\n",
    "Different aggregation functions lead to different information propagation patterns. Let's compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_aggregations(G, num_layers=2):\n",
    "    \"\"\"\n",
    "    Compare how different aggregation functions affect message passing.\n",
    "    \"\"\"\n",
    "    aggregations = ['mean', 'sum', 'max']\n",
    "    results = {}\n",
    "    \n",
    "    for agg in aggregations:\n",
    "        mp = SimpleMessagePassing(num_nodes=len(G.nodes()), feature_dim=4, aggregation=agg)\n",
    "        for edge in G.edges():\n",
    "            mp.add_edge(edge[0], edge[1])\n",
    "        \n",
    "        features, history = mp.forward(num_layers=num_layers, return_history=True)\n",
    "        results[agg] = history\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "agg_results = compare_aggregations(G, num_layers=3)\n",
    "\n",
    "# Analyze and visualize\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Message Passing with Different Aggregation Functions', fontsize=16, fontweight='bold')\n",
    "\n",
    "pos = nx.spring_layout(G, seed=42, k=2)\n",
    "\n",
    "for row, (agg_type, history) in enumerate(agg_results.items()):\n",
    "    for col, (layer, features) in enumerate(enumerate(history)):\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        node_colors = features[:, 0]\n",
    "        node_colors_norm = (node_colors - node_colors.min()) / (node_colors.max() - node_colors.min() + 1e-8)\n",
    "        \n",
    "        nx.draw_networkx_edges(G, pos, ax=ax, width=1.5, alpha=0.5)\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos, ax=ax,\n",
    "            node_color=node_colors_norm,\n",
    "            node_size=600,\n",
    "            cmap='plasma',\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "        nx.draw_networkx_labels(G, pos, ax=ax, font_size=8)\n",
    "        \n",
    "        ax.set_title(f'{agg_type.upper()} - Layer {layer}', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Aggregation Comparison Complete!')\n",
    "print('Observations:')\n",
    "print('- MEAN: Averages neighbor features, preserves scale')\n",
    "print('- SUM: Accumulates neighbor features, can lead to larger values')\n",
    "print('- MAX: Takes maximum element-wise, emphasizes strongest signals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Receptive Field Visualization\n",
    "\n",
    "The receptive field is the set of nodes that can influence a target node's representation. It grows with each message passing layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_receptive_field(G, target_node, max_distance):\n",
    "    \"\"\"\n",
    "    Compute the receptive field of a target node at different distances.\n",
    "    \"\"\"\n",
    "    receptive_field = {}\n",
    "    \n",
    "    visited = {target_node: 0}\n",
    "    queue = deque([target_node])\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        current_distance = visited[node]\n",
    "        \n",
    "        if current_distance >= max_distance:\n",
    "            continue\n",
    "        \n",
    "        for neighbor in G.neighbors(node):\n",
    "            if neighbor not in visited:\n",
    "                visited[neighbor] = current_distance + 1\n",
    "                queue.append(neighbor)\n",
    "    \n",
    "    for node, distance in visited.items():\n",
    "        if distance not in receptive_field:\n",
    "            receptive_field[distance] = set()\n",
    "        receptive_field[distance].add(node)\n",
    "    \n",
    "    return receptive_field\n",
    "\n",
    "\n",
    "# Visualize receptive fields\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Receptive Field Growth with Message Passing Layers', fontsize=14, fontweight='bold')\n",
    "\n",
    "pos = nx.spring_layout(G, seed=42, k=2)\n",
    "target_node = 0\n",
    "\n",
    "for layer_idx in range(6):\n",
    "    ax = axes[layer_idx // 3, layer_idx % 3]\n",
    "    \n",
    "    rf = compute_receptive_field(G, target_node, max_distance=layer_idx)\n",
    "    \n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        if node == target_node:\n",
    "            node_colors.append('red')\n",
    "        elif node in rf.get(layer_idx, set()):\n",
    "            node_colors.append('lightblue')\n",
    "        elif node in [n for d in range(layer_idx) for n in rf.get(d, set())]:\n",
    "            node_colors.append('lightgreen')\n",
    "        else:\n",
    "            node_colors.append('lightgray')\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, width=1.5, alpha=0.5)\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, ax=ax,\n",
    "        node_color=node_colors,\n",
    "        node_size=800,\n",
    "        edgecolors='black',\n",
    "        linewidths=2\n",
    "    )\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax, font_size=10, font_weight='bold')\n",
    "    \n",
    "    all_in_rf = sum(len(rf.get(d, set())) for d in range(layer_idx + 1))\n",
    "    \n",
    "    ax.set_title(f'Layer {layer_idx}\\n(Size: {all_in_rf})', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='red', label='Target Node (0)'),\n",
    "    mpatches.Patch(facecolor='lightblue', label='New in Layer'),\n",
    "    mpatches.Patch(facecolor='lightgreen', label='Previous Layers'),\n",
    "    mpatches.Patch(facecolor='lightgray', label='Outside')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=4, fontsize=10, bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Receptive Field Analysis for Node {target_node}:')\n",
    "for layer in range(4):\n",
    "    rf = compute_receptive_field(G, target_node, max_distance=layer)\n",
    "    total = sum(len(rf.get(d, set())) for d in range(layer + 1))\n",
    "    print(f'  Layer {layer}: {total} nodes in receptive field')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Over-smoothing Problem\n",
    "\n",
    "A critical challenge in deep GNNs: as layers increase, node representations become increasingly similar, reducing their distinctiveness. This is called **over-smoothing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_over_smoothing(num_layers=10, num_nodes=20):\n",
    "    \"\"\"\n",
    "    Demonstrate the over-smoothing problem in deep GNNs.\n",
    "    \"\"\"\n",
    "    G_random = nx.erdos_renyi_graph(num_nodes, p=0.3, seed=42)\n",
    "    \n",
    "    adj = nx.to_numpy_array(G_random)\n",
    "    adj = adj + np.eye(num_nodes)\n",
    "    row_sum = adj.sum(axis=1, keepdims=True)\n",
    "    adj = adj / row_sum\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    \n",
    "    X = torch.randn(num_nodes, 16)\n",
    "    \n",
    "    feature_distances = []\n",
    "    feature_vars = []\n",
    "    \n",
    "    current_features = X.clone()\n",
    "    feature_distances.append(float('inf'))\n",
    "    feature_vars.append(current_features.var().item())\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        aggregated = torch.matmul(adj, current_features)\n",
    "        \n",
    "        W = torch.randn(16, 16) * 0.01\n",
    "        current_features = torch.matmul(aggregated, W)\n",
    "        current_features = torch.relu(current_features)\n",
    "        current_features = torch.tanh(current_features)\n",
    "        \n",
    "        pairwise_distances = torch.cdist(current_features, current_features)\n",
    "        mask = ~torch.eye(num_nodes, dtype=torch.bool)\n",
    "        avg_distance = pairwise_distances[mask].mean().item()\n",
    "        \n",
    "        feature_distances.append(avg_distance)\n",
    "        feature_vars.append(current_features.var().item())\n",
    "    \n",
    "    return feature_distances, feature_vars\n",
    "\n",
    "\n",
    "feature_distances, feature_vars = demonstrate_over_smoothing(num_layers=15, num_nodes=20)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(range(len(feature_distances)), feature_distances, marker='o', linewidth=2, markersize=6, color='darkblue')\n",
    "ax.fill_between(range(len(feature_distances)), feature_distances, alpha=0.3, color='blue')\n",
    "ax.set_xlabel('Layer', fontsize=12)\n",
    "ax.set_ylabel('Average Pairwise Distance', fontsize=12)\n",
    "ax.set_title('Over-smoothing: Nodes Become More Similar', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axvline(x=5, color='red', linestyle='--', alpha=0.5, label='Problem Zone Begins')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(range(len(feature_vars)), feature_vars, marker='s', linewidth=2, markersize=6, color='darkgreen')\n",
    "ax.fill_between(range(len(feature_vars)), feature_vars, alpha=0.3, color='green')\n",
    "ax.set_xlabel('Layer', fontsize=12)\n",
    "ax.set_ylabel('Feature Variance', fontsize=12)\n",
    "ax.set_title('Feature Variance Decrease with Depth', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Over-smoothing Analysis:')\n",
    "print(f'Initial average distance: {feature_distances[1]:.4f}')\n",
    "print(f'Final average distance: {feature_distances[-1]:.4f}')\n",
    "print(f'Distance decreased by: {(1 - feature_distances[-1]/feature_distances[1]) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Simple GNN Implementation\n",
    "\n",
    "Now let's implement a complete GNN model for node classification using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple yet effective GNN model for node classification.\n",
    "    \n",
    "    Architecture:\n",
    "    Input -> GNN Layer 1 -> ReLU -> GNN Layer 2 -> Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features, out_features, num_layers=2):\n",
    "        \"\"\"\n",
    "        Initialize GNN model.\n",
    "        \n",
    "        Args:\n",
    "            in_features: Input feature dimension\n",
    "            hidden_features: Hidden feature dimension\n",
    "            out_features: Output feature dimension (classes)\n",
    "            num_layers: Number of GNN layers\n",
    "        \"\"\"\n",
    "        super(GNNModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features, hidden_features)\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(num_layers - 2):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_features, hidden_features))\n",
    "        \n",
    "        self.linear2 = nn.Linear(hidden_features, out_features)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, X, adj):\n",
    "        \"\"\"\n",
    "        Forward pass: apply message passing and classification.\n",
    "        \n",
    "        Args:\n",
    "            X: Node features (num_nodes, in_features)\n",
    "            adj: Normalized adjacency matrix (num_nodes, num_nodes)\n",
    "        \n",
    "        Returns:\n",
    "            Node predictions (num_nodes, out_features)\n",
    "        \"\"\"\n",
    "        x = torch.matmul(adj, X)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = torch.matmul(adj, x)\n",
    "            x = hidden_layer(x)\n",
    "            x = self.relu(x)\n",
    "        \n",
    "        x = torch.matmul(adj, x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print('GNNModel class defined successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_graph_dataset(num_nodes=100, num_features=10, num_classes=3, seed=42):\n",
    "    \"\"\"\n",
    "    Create a synthetic graph dataset for node classification.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    G = nx.connected_watts_strogatz_graph(num_nodes, k=4, p=0.3, seed=seed)\n",
    "    \n",
    "    X = torch.randn(num_nodes, num_features)\n",
    "    \n",
    "    y = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    for i in range(num_nodes):\n",
    "        y[i] = (i // (num_nodes // num_classes)) % num_classes\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        X[i, :int(num_features/2)] += float(y[i])\n",
    "    \n",
    "    adj = nx.to_numpy_array(G)\n",
    "    adj = adj + np.eye(num_nodes)\n",
    "    row_sum = adj.sum(axis=1, keepdims=True)\n",
    "    adj = adj / row_sum\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    \n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    indices = np.arange(num_nodes)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_idx = indices[:int(0.6 * num_nodes)]\n",
    "    val_idx = indices[int(0.6 * num_nodes):int(0.8 * num_nodes)]\n",
    "    test_idx = indices[int(0.8 * num_nodes):]\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    return X, y, adj, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "X, y, adj, train_mask, val_mask, test_mask = create_synthetic_graph_dataset(\n",
    "    num_nodes=100, num_features=16, num_classes=3\n",
    ")\n",
    "\n",
    "print(f'Dataset created:')\n",
    "print(f'  Nodes: {X.shape[0]}')\n",
    "print(f'  Features: {X.shape[1]}')\n",
    "print(f'  Classes: {int(y.max().item()) + 1}')\n",
    "print(f'  Train samples: {train_mask.sum().item()}')\n",
    "print(f'  Validation samples: {val_mask.sum().item()}')\n",
    "print(f'  Test samples: {test_mask.sum().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn(model, X, y, adj, train_mask, val_mask, test_mask, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train the GNN model.\n",
    "    \"\"\"\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X, adj)\n",
    "        loss = criterion(logits[train_mask], y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(X, adj)\n",
    "                val_preds = logits[val_mask].argmax(dim=1)\n",
    "                val_acc = (val_preds == y[val_mask]).float().mean().item()\n",
    "                val_accs.append(val_acc)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X, adj)\n",
    "        test_preds = logits[test_mask].argmax(dim=1)\n",
    "        test_acc = (test_preds == y[test_mask]).float().mean().item()\n",
    "    \n",
    "    return train_losses, val_accs, test_acc\n",
    "\n",
    "\n",
    "model = GNNModel(in_features=16, hidden_features=32, out_features=3, num_layers=2)\n",
    "train_losses, val_accs, test_acc = train_gnn(\n",
    "    model, X, y, adj, train_mask, val_mask, test_mask,\n",
    "    epochs=100, lr=0.01\n",
    ")\n",
    "\n",
    "print(f'Training completed!')\n",
    "print(f'Final Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(range(len(train_losses)), train_losses, linewidth=2, color='darkblue')\n",
    "ax.fill_between(range(len(train_losses)), train_losses, alpha=0.3, color='blue')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Training Loss', fontsize=12)\n",
    "ax.set_title('Training Loss Over Epochs', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "val_epochs = [i * 10 for i in range(len(val_accs))]\n",
    "ax.plot(val_epochs, val_accs, marker='o', linewidth=2, markersize=8, color='darkgreen')\n",
    "ax.fill_between(val_epochs, val_accs, alpha=0.3, color='green')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax.set_title('Validation Accuracy Over Epochs', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Training Summary:')\n",
    "print(f'  Initial loss: {train_losses[0]:.4f}')\n",
    "print(f'  Final loss: {train_losses[-1]:.4f}')\n",
    "print(f'  Loss reduction: {(1 - train_losses[-1]/train_losses[0]) * 100:.1f}%')\n",
    "print(f'  Max validation accuracy: {max(val_accs):.4f}')\n",
    "print(f'  Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Key Insights Summary\n",
    "\n",
    "### Message Passing Mechanism\n",
    "- Nodes exchange information with neighbors through edges\n",
    "- Information propagates through multiple layers, creating larger receptive fields\n",
    "- Different aggregation functions (mean, sum, max) lead to different behaviors\n",
    "\n",
    "### Receptive Fields\n",
    "- Layer 0: Node only sees itself\n",
    "- Layer 1: Node sees itself and 1-hop neighbors\n",
    "- Layer k: Node sees all neighbors within k-hops\n",
    "- Deeper models capture longer-range dependencies\n",
    "\n",
    "### Over-smoothing Problem\n",
    "- As layers increase, node representations become similar\n",
    "- Models deeper than 4-6 layers often suffer from this\n",
    "- Solutions: skip connections, residual networks, attention mechanisms\n",
    "\n",
    "### GNN Design Considerations\n",
    "1. **Number of layers**: Balance between receptive field and over-smoothing\n",
    "2. **Aggregation function**: Choose based on problem characteristics\n",
    "3. **Feature dimension**: Larger hidden dimensions capture more information\n",
    "4. **Normalization**: Proper normalization of adjacency matrix is crucial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Exercises\n",
    "\n",
    "### Exercise 1: Implement Attention-based Aggregation\n",
    "\n",
    "Instead of simple mean/sum aggregation, implement attention-based aggregation where neighbors with higher attention weights have more influence.\n",
    "\n",
    "```python\n",
    "class AttentionAggregation:\n",
    "    '''\n",
    "    Implement attention-based message aggregation.\n",
    "    \n",
    "    TODO:\n",
    "    1. For each node, compute attention weights for each neighbor\n",
    "    2. Weights should be learned or based on feature similarity\n",
    "    3. Aggregate messages as weighted sum using attention weights\n",
    "    4. Compare with uniform aggregation\n",
    "    '''\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Exercise 2: Explore the Over-smoothing Trade-off\n",
    "\n",
    "Train GNN models with varying depths (1, 2, 3, 4, 5, 6, 7, 8 layers) and measure:\n",
    "- Training accuracy\n",
    "- Validation accuracy\n",
    "- Feature diversity (variance of node representations)\n",
    "\n",
    "Plot the relationship between depth and performance.\n",
    "\n",
    "### Exercise 3: Implement Skip Connections\n",
    "\n",
    "Add residual connections to prevent over-smoothing:\n",
    "\n",
    "```python\n",
    "class GNNWithSkipConnections(nn.Module):\n",
    "    '''\n",
    "    Implement GNN with skip/residual connections.\n",
    "    \n",
    "    Modify the forward pass to:\n",
    "    h_new = h_old + GNN_layer(h_old)\n",
    "    \n",
    "    This helps maintain diversity in node representations.\n",
    "    '''\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Exercise 4: Design Your Own Graph Dataset\n",
    "\n",
    "Create a meaningful graph dataset where:\n",
    "1. Nodes represent entities (people, papers, molecules, etc.)\n",
    "2. Edges represent relationships\n",
    "3. Node features have semantic meaning\n",
    "4. Labels reflect an interesting classification task\n",
    "\n",
    "Train the GNN on your dataset and analyze the results.\n",
    "\n",
    "### Exercise 5: Analyze Aggregation Function Impact\n",
    "\n",
    "For different graph structures (dense, sparse, community-based):\n",
    "1. Train GNN models with different aggregation functions\n",
    "2. Measure convergence speed\n",
    "3. Compare final accuracy\n",
    "4. Explain why certain aggregations work better for specific graphs\n",
    "\n",
    "### Exercise 6: Challenge - Implement GNN from Scratch\n",
    "\n",
    "Implement a custom GNN class that clearly shows all message passing steps without using pre-built modules where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement a custom GNN from scratch\n",
    "# Fill in the TODO sections\n",
    "\n",
    "class CustomGNNFromScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN implementation that clearly shows all message passing steps.\n",
    "    \n",
    "    TODO: Implement the following methods:\n",
    "    1. message_fn: Compute messages from source nodes\n",
    "    2. aggregate_fn: Aggregate messages at target nodes\n",
    "    3. update_fn: Update node representation using aggregated messages\n",
    "    4. forward: Orchestrate the message passing process\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CustomGNNFromScratch, self).__init__()\n",
    "        self.W_msg = nn.Linear(in_features, out_features)\n",
    "        self.W_upd = nn.Linear(in_features + out_features, out_features)\n",
    "    \n",
    "    def message_fn(self, node_features):\n",
    "        \"\"\"\n",
    "        TODO: Implement message function\n",
    "        Input: node_features (source node features)\n",
    "        Output: message (transformed feature vector)\n",
    "        \"\"\"\n",
    "        return self.W_msg(node_features)\n",
    "    \n",
    "    def aggregate_fn(self, messages):\n",
    "        \"\"\"\n",
    "        TODO: Implement aggregation function\n",
    "        Input: messages (list of message tensors from neighbors)\n",
    "        Output: aggregated_message (single tensor)\n",
    "        \"\"\"\n",
    "        if len(messages) == 0:\n",
    "            return None\n",
    "        return torch.mean(torch.stack(messages), dim=0)\n",
    "    \n",
    "    def update_fn(self, node_feature, aggregated_message):\n",
    "        \"\"\"\n",
    "        TODO: Implement update function\n",
    "        Input: node_feature (original node feature), aggregated_message\n",
    "        Output: updated_feature (new node representation)\n",
    "        \"\"\"\n",
    "        combined = torch.cat([node_feature, aggregated_message], dim=-1)\n",
    "        return self.W_upd(combined)\n",
    "    \n",
    "    def forward(self, X, adj):\n",
    "        \"\"\"\n",
    "        TODO: Implement forward pass using message passing\n",
    "        \n",
    "        Steps:\n",
    "        1. For each node, compute messages to send (message_fn)\n",
    "        2. Pass messages along edges (matrix multiplication with adj)\n",
    "        3. At each node, aggregate received messages (aggregate_fn)\n",
    "        4. Update node features using aggregated messages (update_fn)\n",
    "        \n",
    "        Input: X (node features), adj (adjacency matrix)\n",
    "        Output: updated_X (new node features)\n",
    "        \"\"\"\n",
    "        messages = self.message_fn(X)\n",
    "        aggregated = torch.matmul(adj, messages)\n",
    "        updated = self.W_upd(torch.cat([X, aggregated], dim=-1))\n",
    "        return updated\n",
    "\n",
    "\n",
    "print('CustomGNNFromScratch class template created!')\n",
    "print('Now it\\'s your turn to complete the TODO sections!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: References and Further Reading\n",
    "\n",
    "### Key Papers\n",
    "1. **\"A Comprehensive Survey on Graph Neural Networks\" (Wu et al., 2021)**\n",
    "   - Comprehensive overview of GNN architectures and applications\n",
    "\n",
    "2. **\"Semi-Supervised Classification with Graph Convolutional Networks\" (Kipf & Welling, 2017)**\n",
    "   - Foundational GCN paper introducing normalized message passing\n",
    "\n",
    "3. **\"Graph Attention Networks\" (Veli\u010dkovi\u0107 et al., 2018)**\n",
    "   - Introduces attention mechanism for aggregation\n",
    "\n",
    "4. **\"Over-smoothing Problem in Deep GNNs\" (Li et al., 2018)**\n",
    "   - Theoretical analysis of over-smoothing\n",
    "\n",
    "### Related Concepts\n",
    "- **Graph Convolutional Networks (GCN)**: Specific instantiation of message passing with spectral theory\n",
    "- **Graph Attention Networks (GAT)**: Attention-based aggregation\n",
    "- **GraphSAGE**: Importance sampling-based aggregation\n",
    "- **Message Passing Neural Networks (MPNN)**: General framework for message passing\n",
    "\n",
    "### Applications\n",
    "- Node classification (recommendation systems, social network analysis)\n",
    "- Graph classification (molecular property prediction, network clustering)\n",
    "- Link prediction (knowledge graph completion, friend suggestion)\n",
    "- Graph generation (molecule design, architecture discovery)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}