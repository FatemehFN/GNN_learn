{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8: Advanced Topics & Applications - Hands-On Notebook\n",
    "\n",
    "In this notebook, we'll implement advanced GNN concepts and work through real-world applications including:\n",
    "1. Heterogeneous graphs with multiple node/edge types\n",
    "2. Temporal graph forecasting\n",
    "3. Link prediction in knowledge graphs\n",
    "4. Molecular property prediction\n",
    "5. Recommendation system with GNNs\n",
    "6. Complete end-to-end projects\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement heterogeneous graph neural networks\n",
    "- Handle temporal dynamics in graphs\n",
    "- Perform link prediction and knowledge graph completion\n",
    "- Work with molecular graphs\n",
    "- Build practical recommendation systems\n",
    "- Understand real-world applications and challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "np.random.seed(42)\n",
    "\n",
    "# PyTorch and PyG imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.nn import GCNConv, GraphConv, global_mean_pool, global_add_pool\n",
    "    from torch_geometric.data import Data, DataLoader as GDataLoader\n",
    "    print(\"PyTorch Geometric imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch Geometric not installed. Run: pip install torch-geometric\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Heterogeneous Graphs\n",
    "\n",
    "Let's build a heterogeneous graph representing an academic network with authors, papers, and venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousGraph:\n",
    "    \"\"\"Heterogeneous graph with multiple node and edge types\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.node_types = {}  # node_type -> {node_id: attributes}\n",
    "        self.edge_types = defaultdict(list)  # edge_type -> [(source, target, attributes)]\n",
    "        self.node_id_mapping = {}  # type -> {name: id}\n",
    "        self.node_id_reverse = {}  # type -> {id: name}\n",
    "        self.next_ids = defaultdict(int)\n",
    "    \n",
    "    def add_node(self, node_type, node_name, **attributes):\n",
    "        \"\"\"Add a node to the graph\"\"\"\n",
    "        if node_type not in self.node_types:\n",
    "            self.node_types[node_type] = {}\n",
    "            self.node_id_mapping[node_type] = {}\n",
    "            self.node_id_reverse[node_type] = {}\n",
    "        \n",
    "        if node_name not in self.node_id_mapping[node_type]:\n",
    "            node_id = self.next_ids[node_type]\n",
    "            self.node_types[node_type][node_id] = attributes\n",
    "            self.node_id_mapping[node_type][node_name] = node_id\n",
    "            self.node_id_reverse[node_type][node_id] = node_name\n",
    "            self.next_ids[node_type] += 1\n",
    "    \n",
    "    def add_edge(self, edge_type, source_type, source_name, target_type, target_name, **attributes):\n",
    "        \"\"\"Add an edge to the graph\"\"\"\n",
    "        source_id = self.node_id_mapping[source_type][source_name]\n",
    "        target_id = self.node_id_mapping[target_type][target_name]\n",
    "        self.edge_types[edge_type].append({\n",
    "            'source': (source_type, source_id),\n",
    "            'target': (target_type, target_id),\n",
    "            **attributes\n",
    "        })\n",
    "    \n",
    "    def get_neighbors(self, node_type, node_id, edge_type=None):\n",
    "        \"\"\"Get neighbors of a node\"\"\"\n",
    "        neighbors = defaultdict(list)\n",
    "        \n",
    "        edge_types = [edge_type] if edge_type else self.edge_types.keys()\n",
    "        \n",
    "        for et in edge_types:\n",
    "            for edge in self.edge_types[et]:\n",
    "                if edge['source'][0] == node_type and edge['source'][1] == node_id:\n",
    "                    neighbors[edge['target'][0]].append(edge['target'][1])\n",
    "        \n",
    "        return neighbors\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print graph summary\"\"\"\n",
    "        print(\"Heterogeneous Graph Summary:\")\n",
    "        print(\"\\nNode Types:\")\n",
    "        for node_type, nodes in self.node_types.items():\n",
    "            print(f\"  {node_type}: {len(nodes)} nodes\")\n",
    "        \n",
    "        print(\"\\nEdge Types:\")\n",
    "        for edge_type, edges in self.edge_types.items():\n",
    "            print(f\"  {edge_type}: {len(edges)} edges\")\n\n# Create a sample academic network\nacademic_graph = HeterogeneousGraph()\n\n# Add authors\nauthors = ['Alice', 'Bob', 'Carol', 'Dave']\nfor author in authors:\n",
    "    academic_graph.add_node('author', author, affiliation='MIT')\n",
    "\n# Add papers\npapers = ['Paper-A', 'Paper-B', 'Paper-C', 'Paper-D']\nfor paper in papers:\n",
    "    academic_graph.add_node('paper', paper, year=2023)\n",
    "\n# Add venues\nvenues = ['ICML', 'NeurIPS', 'ICLR']\nfor venue in venues:\n",
    "    academic_graph.add_node('venue', venue)\n",
    "\n# Add \"writes\" edges (author -> paper)\nwrites_edges = [\n",
    "    ('Alice', 'Paper-A'),\n",
    "    ('Alice', 'Paper-B'),\n",
    "    ('Bob', 'Paper-A'),\n",
    "    ('Bob', 'Paper-C'),\n",
    "    ('Carol', 'Paper-B'),\n",
    "    ('Carol', 'Paper-D'),\n",
    "    ('Dave', 'Paper-C'),\n",
    "    ('Dave', 'Paper-D'),\n",
    "]\n",
    "\n",
    "for author, paper in writes_edges:\n",
    "    academic_graph.add_edge('writes', 'author', author, 'paper', paper)\n",
    "\n# Add \"publishes\" edges (paper -> venue)\npublishes_edges = [\n",
    "    ('Paper-A', 'ICML'),\n",
    "    ('Paper-B', 'NeurIPS'),\n",
    "    ('Paper-C', 'ICLR'),\n",
    "    ('Paper-D', 'ICML'),\n",
    "]\n",
    "\n",
    "for paper, venue in publishes_edges:\n",
    "    academic_graph.add_edge('publishes', 'paper', paper, 'venue', venue)\n",
    "\n# Add \"collaborates\" edges (author -> author)\ncollaborates_edges = [\n",
    "    ('Alice', 'Bob'),\n",
    "    ('Bob', 'Dave'),\n",
    "    ('Carol', 'Dave'),\n",
    "    ('Alice', 'Carol'),\n",
    "]\n",
    "\n",
    "for author1, author2 in collaborates_edges:\n",
    "    academic_graph.add_edge('collaborates', 'author', author1, 'author', author2)\n",
    "\n# Print summary\n",
    "academic_graph.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterogeneous Graph Neural Network (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHeterogeneousGNN:\n",
    "    \"\"\"Simplified Heterogeneous GNN for educational purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, node_types, embedding_dim=16):\n",
    "        self.node_types = node_types\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialize node embeddings for each type\n",
    "        self.embeddings = {\n",
    "            node_type: np.random.randn(num_nodes, embedding_dim) * 0.01\n",
    "            for node_type, num_nodes in node_types.items()\n",
    "        }\n",
    "        \n",
    "        # Type-specific transformation matrices\n",
    "        self.transforms = {\n",
    "            node_type: np.random.randn(embedding_dim, embedding_dim) * 0.01\n",
    "            for node_type in node_types\n",
    "        }\n",
    "    \n",
    "    def aggregate_neighbors(self, graph, node_type, node_id, edge_type):\n",
    "        \"\"\"Aggregate information from neighbors via specific edge type\"\"\"\n",
    "        neighbors = graph.get_neighbors(node_type, node_id, edge_type)\n",
    "        \n",
    "        aggregated = np.zeros(self.embedding_dim)\n",
    "        count = 0\n",
    "        \n",
    "        for neighbor_type, neighbor_ids in neighbors.items():\n",
    "            for neighbor_id in neighbor_ids:\n",
    "                aggregated += self.embeddings[neighbor_type][neighbor_id]\n",
    "                count += 1\n",
    "        \n",
    "        if count > 0:\n",
    "            aggregated /= count\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def forward(self, graph, num_hops=1):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        for hop in range(num_hops):\n",
    "            new_embeddings = {node_type: [] for node_type in self.node_types}\n",
    "            \n",
    "            for node_type in self.node_types:\n",
    "                for node_id in range(self.node_types[node_type]):\n",
    "                    # Aggregate from different edge types\n",
    "                    aggregated_features = {}\n",
    "                    \n",
    "                    for edge_type in graph.edge_types:\n",
    "                        agg = self.aggregate_neighbors(graph, node_type, node_id, edge_type)\n",
    "                        aggregated_features[edge_type] = agg\n",
    "                    \n",
    "                    # Combine aggregations and apply transformation\n",
    "                    combined = np.mean(list(aggregated_features.values()), axis=0) if aggregated_features else np.zeros(self.embedding_dim)\n",
    "                    combined += self.embeddings[node_type][node_id]\n",
    "                    \n",
    "                    new_embedding = np.tanh(combined @ self.transforms[node_type])\n",
    "                    new_embeddings[node_type].append(new_embedding)\n",
    "            \n",
    "            # Update embeddings\n",
    "            for node_type in self.node_types:\n",
    "                self.embeddings[node_type] = np.array(new_embeddings[node_type])\n",
    "    \n",
    "    def get_embedding(self, node_type, node_id):\n",
    "        \"\"\"Get embedding for a specific node\"\"\"\n",
    "        return self.embeddings[node_type][node_id]\n\n# Create and run the HeterogeneousGNN\nnode_types = {\n",
    "    'author': len(authors),\n",
    "    'paper': len(papers),\n",
    "    'venue': len(venues),\n",
    "}\n",
    "\nhgnn = SimpleHeterogeneousGNN(node_types, embedding_dim=8)\n",
    "\nprint(\"Initial embeddings shape:\")\n",
    "for node_type, embedding in hgnn.embeddings.items():\n",
    "    print(f\"  {node_type}: {embedding.shape}\")\n",
    "\n# Forward pass\nhgnn.forward(academic_graph, num_hops=2)\n",
    "\nprint(\"\\nAfter 2 hops:\")\nfor node_type, embedding in hgnn.embeddings.items():\n",
    "    print(f\"  {node_type}: {embedding.shape}\")\n",
    "    print(f\"    Mean: {embedding.mean():.4f}, Std: {embedding.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-path based Similarity\n",
    "\n",
    "In heterogeneous networks, meta-paths capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths(graph, start_node, start_type, target_type, max_length=4, current_path=None):\n",
    "    \"\"\"Find all paths from start_node to nodes of target_type\"\"\"\n",
    "    if current_path is None:\n",
    "        current_path = [(start_type, start_node)]\n",
    "    \n",
    "    paths = []\n",
    "    \n",
    "    # If we reached target type, record the path\n",
    "    if current_path[-1][0] == target_type and len(current_path) > 1:\n",
    "        paths.append(current_path.copy())\n",
    "    \n",
    "    # Continue exploring if path length allows\n",
    "    if len(current_path) < max_length:\n",
    "        neighbors = graph.get_neighbors(current_path[-1][0], current_path[-1][1])\n",
    "        for neighbor_type, neighbor_ids in neighbors.items():\n",
    "            for neighbor_id in neighbor_ids:\n",
    "                new_node = (neighbor_type, neighbor_id)\n",
    "                if new_node not in current_path:  # Avoid cycles\n",
    "                    new_paths = find_paths(graph, neighbor_id, neighbor_type, target_type, max_length, current_path + [new_node])\n",
    "                    paths.extend(new_paths)\n",
    "    \n",
    "    return paths\n\n# Find paths from authors to other authors (collaboration paths)\nprint(\"Collaboration paths (Author -> ... -> Author):\")\nfor author_name in authors[:2]:\n",
    "    author_id = academic_graph.node_id_mapping['author'][author_name]\n",
    "    paths = find_paths(academic_graph, author_id, 'author', 'author', max_length=4)\n",
    "    print(f\"\\n{author_name}:\")\n",
    "    for path in paths:\n",
    "        path_str = \" → \".join([academic_graph.node_id_reverse[node_type].get(node_id, str(node_id)) \n",
    "                               for node_type, node_id in path])\n",
    "        print(f\"  {path_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Temporal Graphs\n",
    "\n",
    "Model dynamic graphs that evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGraph:\n",
    "    \"\"\"Graph that evolves over time\"\"\"\n",
    "    \n",
    "    def __init__(self, time_steps=10):\n",
    "        self.time_steps = time_steps\n",
    "        self.snapshots = [nx.DiGraph() for _ in range(time_steps)]\n",
    "        self.node_features = {}  # (time, node) -> features\n",
    "    \n",
    "    def add_node(self, time, node, **attributes):\n",
    "        \"\"\"Add node at specific time\"\"\"\n",
    "        self.snapshots[time].add_node(node, **attributes)\n",
    "        self.node_features[(time, node)] = attributes\n",
    "    \n",
    "    def add_edge(self, time, source, target, **attributes):\n",
    "        \"\"\"Add edge at specific time\"\"\"\n",
    "        self.snapshots[time].add_edge(source, target, **attributes)\n",
    "    \n",
    "    def get_snapshot(self, time):\n",
    "        \"\"\"Get graph snapshot at time t\"\"\"\n",
    "        return self.snapshots[time]\n    \n",
    "    def get_temporal_neighbors(self, node, time, k_hops=1):\n",
    "        \"\"\"Get neighbors considering temporal context\"\"\"\n",
    "        temporal_neighbors = {}\n",
    "        \n",
    "        for t in range(max(0, time - k_hops), time + 1):\n",
    "            snapshot = self.snapshots[t]\n",
    "            if node in snapshot:\n",
    "                neighbors = list(snapshot.successors(node))\n",
    "                temporal_neighbors[t] = neighbors\n",
    "        \n",
    "        return temporal_neighbors\n\n# Create a temporal social network\nprint(\"Creating temporal social network...\\n\")\n",
    "temporal_net = TemporalGraph(time_steps=5)\n",
    "\n# Nodes (users) appear and disappear over time\nnodes = ['User-A', 'User-B', 'User-C', 'User-D']\n",
    "\n# Add nodes over time with activity\nfor t in range(5):\n",
    "    for node in nodes:\n",
    "        if np.random.random() > 0.3:  # 70% chance node is active at time t\n",
    "            temporal_net.add_node(t, node, activity_level=np.random.randint(1, 10))\n",
    "\n# Add edges (follows) that evolve over time\nnp.random.seed(42)\nfor t in range(5):\n",
    "    active_nodes = list(temporal_net.snapshots[t].nodes())\n",
    "    \n",
    "    for i, source in enumerate(active_nodes):\n",
    "        # Each node follows 1-2 others\n",
    "        num_follows = np.random.randint(1, 3)\n",
    "        targets = np.random.choice([n for n in active_nodes if n != source], \n",
    "                                   size=min(num_follows, len(active_nodes)-1), \n",
    "                                   replace=False)\n",
    "        \n",
    "        for target in targets:\n",
    "            temporal_net.add_edge(t, source, target, weight=np.random.random())\n\n# Print temporal statistics\nprint(\"Temporal Network Statistics:\")\nfor t in range(5):\n",
    "    snapshot = temporal_net.get_snapshot(t)\n",
    "    print(f\"  Time {t}: {snapshot.number_of_nodes()} nodes, {snapshot.number_of_edges()} edges\")\n\nprint(\"\\nTemporal neighbors of User-A:\")\nfor t in range(5):\n",
    "    temporal_neighbors = temporal_net.get_temporal_neighbors('User-A', t)\n",
    "    if temporal_neighbors:\n",
    "        print(f\"  Time {t}: {temporal_neighbors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Graph Forecasting\n",
    "\n",
    "Predict future graph structure based on historical snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGraphForecaster:\n",
    "    \"\"\"Simple temporal graph forecaster using RNN-like logic\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=16):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory = {}  # node -> hidden state\n",
    "    \n",
    "    def update_memory(self, node, edges_in, edges_out):\n",
    "        \"\"\"Update node memory based on interactions\"\"\"\n",
    "        if node not in self.memory:\n",
    "            self.memory[node] = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        # Simple update: count of recent edges influences memory\n",
    "        activity = np.zeros(self.hidden_dim)\n",
    "        activity[0] = len(edges_in) / 10.0  # Incoming edges\n",
    "        activity[1] = len(edges_out) / 10.0  # Outgoing edges\n",
    "        \n",
    "        # Update with exponential moving average\n",
    "        self.memory[node] = 0.7 * self.memory[node] + 0.3 * activity\n",
    "    \n",
    "    def predict_edge_probability(self, source, target):\n",
    "        \"\"\"Predict probability of edge existing\"\"\"\n",
    "        if source not in self.memory or target not in self.memory:\n",
    "            return 0.5\n",
    "        \n",
    "        # Similarity in memory states correlates with edge probability\n",
    "        similarity = np.dot(self.memory[source], self.memory[target])\n",
    "        similarity = np.clip(similarity, 0, 1)\n",
    "        return similarity\n    \n",
    "    def forecast(self, temporal_graph, forecast_time):\n",
    "        \"\"\"Forecast edges at future time\"\"\"\n",
    "        # Use historical snapshots to update memory\n",
    "        for t in range(min(forecast_time, temporal_graph.time_steps)):\n",
    "            snapshot = temporal_graph.get_snapshot(t)\n",
    "            \n",
    "            for node in snapshot.nodes():\n",
    "                edges_in = list(snapshot.predecessors(node))\n",
    "                edges_out = list(snapshot.successors(node))\n",
    "                self.update_memory(node, edges_in, edges_out)\n",
    "        \n",
    "        # Predict future edges\n",
    "        all_nodes = list(self.memory.keys())\n",
    "        predicted_edges = []\n",
    "        \n",
    "        for source in all_nodes:\n",
    "            for target in all_nodes:\n",
    "                if source != target:\n",
    "                    prob = self.predict_edge_probability(source, target)\n",
    "                    if prob > 0.5:\n",
    "                        predicted_edges.append((source, target, prob))\n",
    "        \n",
    "        return predicted_edges\n\n# Forecast future temporal graph\nforecaster = TemporalGraphForecaster(hidden_dim=4)\npredicted_edges = forecaster.forecast(temporal_net, forecast_time=4)\n\nprint(\"\\nForecast for time=5:\")\nprint(f\"Predicted {len(predicted_edges)} edges:\")\nfor source, target, prob in sorted(predicted_edges, key=lambda x: x[2], reverse=True)[:10]:\n",
    "    print(f\"  {source} -> {target}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Link Prediction and Knowledge Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small knowledge graph\nclass KnowledgeGraph:\n",
    "    \"\"\"Simple knowledge graph for entity-relation-entity triples\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entities = set()\n",
    "        self.relations = set()\n",
    "        self.triples = set()  # (head, relation, tail)\n",
    "        self.entity_to_id = {}\n",
    "        self.relation_to_id = {}\n",
    "        self.next_entity_id = 0\n",
    "        self.next_relation_id = 0\n",
    "    \n",
    "    def add_entity(self, entity):\n",
    "        if entity not in self.entity_to_id:\n",
    "            self.entity_to_id[entity] = self.next_entity_id\n",
    "            self.next_entity_id += 1\n",
    "        self.entities.add(entity)\n",
    "    \n",
    "    def add_relation(self, relation):\n",
    "        if relation not in self.relation_to_id:\n",
    "            self.relation_to_id[relation] = self.next_relation_id\n",
    "            self.next_relation_id += 1\n",
    "        self.relations.add(relation)\n",
    "    \n",
    "    def add_triple(self, head, relation, tail):\n",
    "        self.add_entity(head)\n",
    "        self.add_entity(tail)\n",
    "        self.add_relation(relation)\n",
    "        self.triples.add((head, relation, tail))\n",
    "    \n",
    "    def get_triples_with_head(self, head):\n",
    "        return [(r, t) for (h, r, t) in self.triples if h == head]\n",
    "    \n",
    "    def get_triples_with_tail(self, tail):\n",
    "        return [(h, r) for (h, r, t) in self.triples if t == tail]\n",
    "\n# Create knowledge graph about famous scientists\nkg = KnowledgeGraph()\n\n# Add facts\nfacts = [\n",
    "    ('Albert Einstein', 'born_in', 'Ulm'),\n",
    "    ('Albert Einstein', 'nationality', 'German'),\n",
    "    ('Marie Curie', 'born_in', 'Warsaw'),\n",
    "    ('Marie Curie', 'nationality', 'Polish'),\n",
    "    ('Marie Curie', 'collaborated_with', 'Albert Einstein'),\n",
    "    ('Alan Turing', 'born_in', 'London'),\n",
    "    ('Alan Turing', 'nationality', 'British'),\n",
    "    ('Alan Turing', 'contributed_to', 'Computer Science'),\n",
    "    ('Albert Einstein', 'contributed_to', 'Physics'),\n",
    "    ('Marie Curie', 'contributed_to', 'Chemistry'),\n",
    "    ('Ulm', 'country', 'Germany'),\n",
    "    ('Warsaw', 'country', 'Poland'),\n",
    "    ('London', 'country', 'United Kingdom'),\n",
    "]\n",
    "\n",
    "for head, relation, tail in facts:\n",
    "    kg.add_triple(head, relation, tail)\n",
    "\n",
    "print(f\"Knowledge Graph: {len(kg.entities)} entities, {len(kg.relations)} relations, {len(kg.triples)} triples\")\n",
    "print(f\"\\nEntities: {sorted(kg.entities)}\")\n",
    "print(f\"\\nRelations: {sorted(kg.relations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransE: Translation-based Knowledge Graph Embeddings"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransEModel:\n",
    "    \"\"\"TransE model for knowledge graph embeddings\n",
    "    \n",
    "    Key idea: h + r ≈ t (in embedding space)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kg, embedding_dim=20, learning_rate=0.01):\n",
    "        self.kg = kg\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.margin = 1.0  # Margin for ranking loss\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.entity_embeddings = {}\n",
    "        self.relation_embeddings = {}\n",
    "        \n",
    "        for entity in kg.entities:\n",
    "            self.entity_embeddings[entity] = np.random.randn(embedding_dim) * 0.01\n",
    "        \n",
    "        for relation in kg.relations:\n",
    "            self.relation_embeddings[relation] = np.random.randn(embedding_dim) * 0.01\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        for entity in self.entity_embeddings:\n",
    "            norm = np.linalg.norm(self.entity_embeddings[entity])\n",
    "            if norm > 0:\n",
    "                self.entity_embeddings[entity] /= norm\n",
    "    \n",
    "    def score_triple(self, head, relation, tail):\n",
    "        \"\"\"Score a triple: lower score means more plausible\"\"\"\n",
    "        h = self.entity_embeddings[head]\n",
    "        r = self.relation_embeddings[relation]\n",
    "        t = self.entity_embeddings[tail]\n",
    "        \n",
    "        # TransE score: distance between h+r and t\n",
    "        return np.linalg.norm(h + r - t)\n",
    "    \n",
    "    def train_epoch(self, num_iterations=100):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        triples = list(self.kg.triples)\n",
    "        \n",
    "        for _ in range(num_iterations):\n",
    "            # Sample positive triple\n",
    "            pos_triple = triples[np.random.randint(len(triples))]\n",
    "            h_pos, r_pos, t_pos = pos_triple\n",
    "            \n",
    "            # Generate negative triple by corrupting\n",
    "            if np.random.random() > 0.5:\n",
    "                # Replace head\n",
    "                h_neg = np.random.choice(list(self.kg.entities))\n",
    "                neg_triple = (h_neg, r_pos, t_pos)\n",
    "            else:\n",
    "                # Replace tail\n",
    "                t_neg = np.random.choice(list(self.kg.entities))\n",
    "                neg_triple = (h_pos, r_pos, t_neg)\n",
    "            \n",
    "            # Skip if negative triple is actually positive\n",
    "            if neg_triple in self.kg.triples:\n",
    "                continue\n",
    "            \n",
    "            # Calculate scores\n",
    "            pos_score = self.score_triple(*pos_triple)\n",
    "            neg_score = self.score_triple(*neg_triple)\n",
    "            \n",
    "            # Margin ranking loss\n",
    "            loss = max(0, pos_score - neg_score + self.margin)\n",
    "            \n",
    "            if loss > 0:\n",
    "                # Update embeddings (simplified gradient)\n",
    "                h_pos_emb = self.entity_embeddings[h_pos]\n",
    "                r_emb = self.relation_embeddings[r_pos]\n",
    "                t_pos_emb = self.entity_embeddings[t_pos]\n",
    "                \n",
    "                # Positive triple gradient\n",
    "                pred = h_pos_emb + r_emb - t_pos_emb\n",
    "                grad = 2 * pred\n",
    "                \n",
    "                # Update\n",
    "                self.entity_embeddings[h_pos] -= self.learning_rate * grad\n",
    "                self.relation_embeddings[r_pos] -= self.learning_rate * grad\n",
    "                self.entity_embeddings[t_pos] += self.learning_rate * grad\n",
    "    \n",
    "    def get_embedding(self, entity_or_relation):\n",
    "        \"\"\"Get embedding for entity or relation\"\"\"\n",
    "        if entity_or_relation in self.entity_embeddings:\n",
    "            return self.entity_embeddings[entity_or_relation]\n",
    "        else:\n",
    "            return self.relation_embeddings[entity_or_relation]\n\n# Train TransE model\nprint(\"Training TransE model...\")\ntransE = TransEModel(kg, embedding_dim=10, learning_rate=0.01)\n\nfor epoch in range(50):\n",
    "    transE.train_epoch(num_iterations=50)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Evaluate on some triples\n",
    "        avg_score = np.mean([transE.score_triple(h, r, t) for h, r, t in kg.triples])\n",
    "        print(f\"Epoch {epoch+1}: Avg positive score = {avg_score:.4f}\")\n",
    "\nprint(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction in Knowledge Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_missing_links(transE_model, kg, candidate_relations, top_k=5):\n",
    "    \"\"\"Predict missing links in knowledge graph\"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    for relation in candidate_relations:\n",
    "        candidates = []\n",
    "        \n",
    "        # For each entity pair, score if they should have this relation\n",
    "        for head in kg.entities:\n",
    "            for tail in kg.entities:\n",
    "                if head != tail and (head, relation, tail) not in kg.triples:\n",
    "                    score = transE_model.score_triple(head, relation, tail)\n",
    "                    candidates.append((head, tail, score))\n",
    "        \n",
    "        # Top k candidates with lowest scores (most plausible)\n",
    "        candidates.sort(key=lambda x: x[2])\n",
    "        predictions[relation] = candidates[:top_k]\n",
    "    \n",
    "    return predictions\n\n# Predict missing links\npredictions = predict_missing_links(transE, kg, ['born_in', 'nationality', 'contributed_to'], top_k=3)\n\nprint(\"Predicted Missing Links:\")\nfor relation, predicted_triples in predictions.items():\n",
    "    print(f\"\\nRelation: {relation}\")\n",
    "    for head, tail, score in predicted_triples:\n",
    "        actual = \"✓\" if (head, relation, tail) in kg.triples else \"?\"\n",
    "        print(f\"  {actual} ({head}, {relation}, {tail}) - score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Molecular Property Prediction\n",
    "\n",
    "Predict molecular properties from graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_molecular_graph(smiles_string=None, num_atoms=5):\n",
    "    \"\"\"Create a simple molecular graph\n",
    "    \n",
    "    For simplicity, we'll create random graphs representing molecules\n",
    "    In practice, you'd use RDKit to parse SMILES strings\n",
    "    \"\"\"\n",
    "    \n",
    "    if smiles_string:\n",
    "        # In real scenario, parse SMILES with RDKit\n",
    "        # For now, create a simple graph\n",
    "        pass\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add atoms (nodes)\n",
    "    atom_types = ['C', 'N', 'O', 'S', 'P']\n",
    "    for i in range(num_atoms):\n",
    "        atom_type = atom_types[i % len(atom_types)]\n",
    "        # Features: atomic_number, hybridization, etc.\n",
    "        atomic_num = {'C': 6, 'N': 7, 'O': 8, 'S': 16, 'P': 15}[atom_type]\n",
    "        G.add_node(i, atom=atom_type, atomic_num=atomic_num)\n",
    "    \n",
    "    # Add bonds (edges)\n",
    "    bond_types = [1, 2, 3]  # single, double, triple\n",
    "    \n",
    "    # Create connected structure\n",
    "    for i in range(num_atoms - 1):\n",
    "        bond_type = bond_types[np.random.randint(len(bond_types))]\n",
    "        G.add_edge(i, i+1, bond_type=bond_type)\n",
    "    \n",
    "    # Add some random bonds\n",
    "    for _ in range(num_atoms // 2):\n",
    "        i, j = np.random.choice(num_atoms, 2, replace=False)\n",
    "        if not G.has_edge(i, j):\n",
    "            bond_type = bond_types[np.random.randint(len(bond_types))]\n",
    "            G.add_edge(i, j, bond_type=bond_type)\n",
    "    \n",
    "    return G\n\n# Create sample molecules\nmolecules = [create_molecular_graph(num_atoms=np.random.randint(4, 10)) for _ in range(20)]\n\nprint(f\"Created {len(molecules)} molecules\")\nfor i, mol in enumerate(molecules[:3]):\n",
    "    print(f\"  Molecule {i}: {mol.number_of_nodes()} atoms, {mol.number_of_edges()} bonds\")\n\n# Visualize one molecule\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nfor idx, ax in enumerate(axes):\n",
    "    mol = molecules[idx]\n",
    "    pos = nx.spring_layout(mol, seed=42)\n",
    "    \n",
    "    # Node colors by atom type\n",
    "    atom_colors = {'C': 'gray', 'N': 'blue', 'O': 'red', 'S': 'yellow', 'P': 'orange'}\n",
    "    node_colors = [atom_colors[mol.nodes[node]['atom']] for node in mol.nodes()]\n",
    "    \n",
    "    nx.draw(mol, pos, ax=ax, with_labels=True, node_color=node_colors,\n",
    "            node_size=500, font_size=10, font_weight='bold', edge_color='gray')\n",
    "    ax.set_title(f'Molecule {idx}: {mol.number_of_nodes()} atoms')\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular Property Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularGNN:\n",
    "    \"\"\"Simple GNN for molecular property prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=16, num_layers=2):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.atom_embeddings = {}\n",
    "    \n",
    "    def extract_features(self, mol):\n",
    "        \"\"\"Extract node and edge features from molecule\"\"\"\n",
    "        num_atoms = mol.number_of_nodes()\n",
    "        \n",
    "        # Node features: one-hot encoding of atom type + degree\n",
    "        atom_types = set([mol.nodes[n]['atom'] for n in mol.nodes()])\n",
    "        atom_type_to_idx = {at: i for i, at in enumerate(sorted(atom_types))}\n",
    "        \n",
    "        node_features = []\n",
    "        for node in range(num_atoms):\n",
    "            feat = [0] * (len(atom_type_to_idx) + 1)  # one-hot + degree\n",
    "            atom_type = mol.nodes[node]['atom']\n",
    "            feat[atom_type_to_idx[atom_type]] = 1\n",
    "            feat[-1] = mol.degree(node) / num_atoms  # normalized degree\n",
    "            node_features.append(feat)\n",
    "        \n",
    "        # Edge features: bond type\n",
    "        edge_features = []\n",
    "        edge_index = []\n",
    "        for u, v in mol.edges():\n",
    "            bond_type = mol[u][v].get('bond_type', 1)\n",
    "            edge_features.append([bond_type])\n",
    "            edge_index.append((u, v))\n",
    "            edge_index.append((v, u))  # undirected\n",
    "        \n",
    "        return np.array(node_features), np.array(edge_index), np.array(edge_features)\n",
    "    \n",
    "    def message_passing(self, mol, node_embeddings):\n",
    "        \"\"\"Perform message passing aggregation\"\"\"\n",
    "        new_embeddings = []\n",
    "        \n",
    "        for node in mol.nodes():\n",
    "            # Aggregate neighbor embeddings\n",
    "            neighbor_embeddings = [node_embeddings[neighbor] \n",
    "                                   for neighbor in mol.neighbors(node)]\n",
    "            \n",
    "            if neighbor_embeddings:\n",
    "                aggregated = np.mean(neighbor_embeddings, axis=0)\n",
    "            else:\n",
    "                aggregated = np.zeros_like(node_embeddings[node])\n",
    "            \n",
    "            # Combine with self embedding\n",
    "            new_emb = np.tanh(node_embeddings[node] + aggregated)\n",
    "            new_embeddings.append(new_emb)\n",
    "        \n",
    "        return new_embeddings\n",
    "    \n",
    "    def forward(self, mol):\n",
    "        \"\"\"Forward pass: get graph representation\"\"\"\n",
    "        node_features, edge_index, edge_features = self.extract_features(mol)\n",
    "        \n",
    "        # Initialize embeddings from features\n",
    "        node_embeddings = node_features.copy()\n",
    "        \n",
    "        # Message passing layers\n",
    "        for _ in range(self.num_layers):\n",
    "            node_embeddings = self.message_passing(mol, node_embeddings)\n",
    "        \n",
    "        # Global pooling: mean of all node embeddings\n",
    "        graph_embedding = np.mean(node_embeddings, axis=0)\n",
    "        \n",
    "        return graph_embedding, node_embeddings\n\n# Generate synthetic molecular properties\nprint(\"Generating synthetic molecular properties...\")\n\nmol_model = MolecularGNN(hidden_dim=8, num_layers=2)\n\n# Generate properties based on graph structure\nmolecular_properties = {}\nfor i, mol in enumerate(molecules):\n",
    "    graph_emb, node_embs = mol_model.forward(mol)\n",
    "    \n",
    "    # Synthetic properties based on graph features\n",
    "    num_atoms = mol.number_of_nodes()\n",
    "    num_bonds = mol.number_of_edges()\n",
    "    density = 2 * num_bonds / (num_atoms * (num_atoms - 1))\n",
    "    \n",
    "    # Predicted properties\n",
    "    solubility = 5 - density * 10 + np.random.randn() * 0.5\n",
    "    toxicity = density * 0.5 + np.random.randn() * 0.2\n",
    "    \n",
    "    molecular_properties[i] = {\n",
    "        'solubility': solubility,\n",
    "        'toxicity': toxicity,\n",
    "        'num_atoms': num_atoms,\n",
    "        'num_bonds': num_bonds,\n",
    "        'density': density\n",
    "    }\n",
    "\nprint(\"Sample molecular properties:\")\nfor i in range(3):\n",
    "    props = molecular_properties[i]\n",
    "    print(f\"\\nMolecule {i}:\")\n",
    "    print(f\"  Atoms: {props['num_atoms']}, Bonds: {props['num_bonds']}, Density: {props['density']:.3f}\")\n",
    "    print(f\"  Solubility: {props['solubility']:.2f}, Toxicity: {props['toxicity']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Molecular Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze molecular properties\ndf_mols = pd.DataFrame(molecular_properties).T\n",
    "\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n# Distribution of properties\naxes[0, 0].hist(df_mols['solubility'], bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Solubility')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Solubility')\n",
    "\naxes[0, 1].hist(df_mols['toxicity'], bins=10, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Toxicity')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Toxicity')\n",
    "\n# Scatter plots\naxes[1, 0].scatter(df_mols['num_atoms'], df_mols['solubility'], alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Number of Atoms')\n",
    "axes[1, 0].set_ylabel('Solubility')\n",
    "axes[1, 0].set_title('Solubility vs Molecular Size')\n",
    "\naxes[1, 1].scatter(df_mols['density'], df_mols['toxicity'], alpha=0.6, color='red')\n",
    "axes[1, 1].set_xlabel('Density')\n",
    "axes[1, 1].set_ylabel('Toxicity')\n",
    "axes[1, 1].set_title('Toxicity vs Connectivity')\n",
    "\nplt.tight_layout()\nplt.show()\n",
    "\nprint(\"\\nMolecular Properties Statistics:\")\nprint(df_mols.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Recommendation System with GNNs\n",
    "\n",
    "Build a recommendation system using user-item bipartite graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic user-item interaction data\nnp.random.seed(42)\n",
    "\nnum_users = 20\nnum_items = 30\nnum_interactions = 150\n",
    "\n# Create user-item bipartite graph\nrecommendation_graph = nx.Graph()\n",
    "\n# Add users\nfor u in range(num_users):\n",
    "    recommendation_graph.add_node(f'user_{u}', node_type='user')\n",
    "\n# Add items\nfor i in range(num_items):\n",
    "    recommendation_graph.add_node(f'item_{i}', node_type='item')\n",
    "\n# Add interactions (ratings)\ninteraction_matrix = np.zeros((num_users, num_items))\nfor _ in range(num_interactions):\n",
    "    user = np.random.randint(num_users)\n",
    "    item = np.random.randint(num_items)\n",
    "    \n",
    "    if interaction_matrix[user, item] == 0:  # Not already rated\n",
    "        rating = np.random.randint(1, 6)  # 1-5 star rating\n",
    "        interaction_matrix[user, item] = rating\n",
    "        recommendation_graph.add_edge(f'user_{user}', f'item_{item}', rating=rating)\n",
    "\nprint(f\"User-Item Graph:\")\nprint(f\"  Users: {num_users}\")\nprint(f\"  Items: {num_items}\")\nprint(f\"  Interactions: {recommendation_graph.number_of_edges()}\")\nprint(f\"  Sparsity: {1 - recommendation_graph.number_of_edges() / (num_users * num_items):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering with GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationGNN:\n",
    "    \"\"\"GNN-based recommendation system\"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, embedding_dim=16):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.user_embeddings = np.random.randn(num_users, embedding_dim) * 0.01\n",
    "        self.item_embeddings = np.random.randn(num_items, embedding_dim) * 0.01\n",
    "    \n",
    "    def aggregate_item_neighborhoods(self, user_id, graph):\n",
    "        \"\"\"Aggregate item embeddings from user's neighbors\"\"\"\n",
    "        neighbors = list(graph.neighbors(f'user_{user_id}'))\n",
    "        \n",
    "        if not neighbors:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        # Get item embeddings\n",
    "        neighbor_embeddings = []\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor.startswith('item_'):\n",
    "                item_id = int(neighbor.split('_')[1])\n",
    "                neighbor_embeddings.append(self.item_embeddings[item_id])\n",
    "        \n",
    "        if neighbor_embeddings:\n",
    "            return np.mean(neighbor_embeddings, axis=0)\n",
    "        return np.zeros(self.embedding_dim)\n",
    "    \n",
    "    def aggregate_user_neighborhoods(self, item_id, graph):\n",
    "        \"\"\"Aggregate user embeddings from item's neighbors\"\"\"\n",
    "        neighbors = list(graph.neighbors(f'item_{item_id}'))\n",
    "        \n",
    "        if not neighbors:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        # Get user embeddings\n",
    "        neighbor_embeddings = []\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor.startswith('user_'):\n",
    "                user_id = int(neighbor.split('_')[1])\n",
    "                neighbor_embeddings.append(self.user_embeddings[user_id])\n",
    "        \n",
    "        if neighbor_embeddings:\n",
    "            return np.mean(neighbor_embeddings, axis=0)\n",
    "        return np.zeros(self.embedding_dim)\n",
    "    \n",
    "    def update_embeddings(self, graph, num_iterations=2):\n",
    "        \"\"\"Update embeddings using message passing\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            # Update user embeddings\n",
    "            new_user_embeddings = []\n",
    "            for user_id in range(self.num_users):\n",
    "                aggregated = self.aggregate_item_neighborhoods(user_id, graph)\n",
    "                new_emb = np.tanh(self.user_embeddings[user_id] + aggregated)\n",
    "                new_user_embeddings.append(new_emb)\n",
    "            self.user_embeddings = np.array(new_user_embeddings)\n",
    "            \n",
    "            # Update item embeddings\n",
    "            new_item_embeddings = []\n",
    "            for item_id in range(self.num_items):\n",
    "                aggregated = self.aggregate_user_neighborhoods(item_id, graph)\n",
    "                new_emb = np.tanh(self.item_embeddings[item_id] + aggregated)\n",
    "                new_item_embeddings.append(new_emb)\n",
    "            self.item_embeddings = np.array(new_item_embeddings)\n",
    "    \n",
    "    def predict_rating(self, user_id, item_id):\n",
    "        \"\"\"Predict rating for user-item pair\"\"\"\n",
    "        user_emb = self.user_embeddings[user_id]\n",
    "        item_emb = self.item_embeddings[item_id]\n",
    "        \n",
    "        # Simple scoring: dot product + sigmoid to scale 0-5\n",
    "        score = np.dot(user_emb, item_emb)\n",
    "        rating = 2.5 + 2.5 * np.tanh(score)  # Scale to approximately 0-5\n",
    "        return np.clip(rating, 1, 5)\n",
    "    \n",
    "    def recommend_for_user(self, user_id, graph, k=5):\n",
    "        \"\"\"Recommend top-k items for a user\"\"\"\n",
    "        # Get items user has already interacted with\n",
    "        rated_items = set()\n",
    "        for neighbor in graph.neighbors(f'user_{user_id}'):\n",
    "            if neighbor.startswith('item_'):\n",
    "                item_id = int(neighbor.split('_')[1])\n",
    "                rated_items.add(item_id)\n",
    "        \n",
    "        # Score all unrated items\n",
    "        scores = []\n",
    "        for item_id in range(self.num_items):\n",
    "            if item_id not in rated_items:\n",
    "                rating = self.predict_rating(user_id, item_id)\n",
    "                scores.append((item_id, rating))\n",
    "        \n",
    "        # Return top-k\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:k]\n\n# Create and train recommendation model\nprint(\"Training Recommendation GNN...\")\nrec_model = RecommendationGNN(num_users, num_items, embedding_dim=8)\nrec_model.update_embeddings(recommendation_graph, num_iterations=5)\nprint(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\nprint(\"Sample Recommendations:\")\nfor user_id in range(min(3, num_users)):\n",
    "    recommendations = rec_model.recommend_for_user(user_id, recommendation_graph, k=5)\n",
    "    print(f\"\\nUser {user_id}:\")\n",
    "    for item_id, predicted_rating in recommendations:\n",
    "        print(f\"  Item {item_id}: {predicted_rating:.2f} stars\")\n\n# Evaluate model on held-out interactions\nprint(\"\\n\" + \"=\"*50)\nprint(\"Model Evaluation\")\nprint(\"=\"*50)\n",
    "\n# Calculate MAE (Mean Absolute Error) on training data\nerrors = []\nfor u, v in recommendation_graph.edges():\n",
    "    if v.startswith('item_'):\n",
    "        user_id = int(u.split('_')[1])\n",
    "        item_id = int(v.split('_')[1])\n",
    "        actual_rating = recommendation_graph[u][v]['rating']\n",
    "        predicted_rating = rec_model.predict_rating(user_id, item_id)\n",
    "        error = abs(actual_rating - predicted_rating)\n",
    "        errors.append(error)\n",
    "\nmae = np.mean(errors)\nprint(f\"Mean Absolute Error (MAE): {mae:.3f}\")\nprint(f\"Root Mean Squared Error (RMSE): {np.sqrt(np.mean(np.array(errors)**2)):.3f}\")\n\n# Calculate sparsity metrics\ntotal_possible = num_users * num_items\nactual_interactions = recommendation_graph.number_of_edges()\nsparsity = 1 - (actual_interactions / total_possible)\nprint(f\"\\nData Sparsity: {sparsity:.3f}\")\nprint(f\"Coverage (items rated): {actual_interactions / total_possible * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: End-to-End Project - Citation Network Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a citation network\nprint(\"Building citation network...\")\n",
    "\ncitation_graph = nx.DiGraph()\n",
    "\n# Create papers (nodes)\nnum_papers = 50\nfor i in range(num_papers):\n",
    "    year = 2015 + (i // 10)  # Papers from 2015-2019\n",
    "    citation_graph.add_node(f'paper_{i}', year=year, citations=0)\n",
    "\n# Add citations (edges) - older papers cite newer papers\nfor i in range(num_papers):\n",
    "    year_i = citation_graph.nodes[f'paper_{i}']['year']\n",
    "    # Each paper cites 2-5 others\n",
    "    num_citations = np.random.randint(2, 6)\n",
    "    \n",
    "    # Can only cite papers from same or earlier years\n",
    "    possible_targets = [j for j in range(num_papers) \n",
    "                       if citation_graph.nodes[f'paper_{j}']['year'] <= year_i \n",
    "                       and i != j]\n",
    "    \n",
    "    if possible_targets:\n",
    "        targets = np.random.choice(possible_targets, \n",
    "                                   size=min(num_citations, len(possible_targets)), \n",
    "                                   replace=False)\n",
    "        for target in targets:\n",
    "            citation_graph.add_edge(f'paper_{i}', f'paper_{target}')\n",
    "            # Update citation count\n",
    "            citation_graph.nodes[f'paper_{target}']['citations'] += 1\n",
    "\nprint(f\"Citation Network: {citation_graph.number_of_nodes()} papers, {citation_graph.number_of_edges()} citations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction: Common Neighbors Baseline"
   ]
  },
  {
   "cell_type": {"execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors_score(graph, node1, node2):\n",
    "    \"\"\"Score based on common neighbors\"\"\"\n",
    "    neighbors1 = set(graph.successors(node1))\n",
    "    neighbors2 = set(graph.predecessors(node2))\n",
    "    \n",
    "    common = len(neighbors1 & neighbors2)\n",
    "    return common\n",
    "\ndef jaccard_similarity(graph, node1, node2):\n",
    "    \"\"\"Jaccard similarity of neighborhoods\"\"\"\n",
    "    neighbors1 = set(graph.successors(node1))\n",
    "    neighbors2 = set(graph.successors(node2))\n",
    "    \n",
    "    if len(neighbors1 | neighbors2) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return len(neighbors1 & neighbors2) / len(neighbors1 | neighbors2)\n",
    "\ndef predict_links(graph, method='common_neighbors', k=10):\n",
    "    \"\"\"Predict top-k missing links\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Consider all pairs not already connected\n",
    "    for node1 in graph.nodes():\n",
    "        for node2 in graph.nodes():\n",
    "            if node1 != node2 and not graph.has_edge(node1, node2):\n",
    "                if method == 'common_neighbors':\n",
    "                    score = common_neighbors_score(graph, node1, node2)\n",
    "                elif method == 'jaccard':\n",
    "                    score = jaccard_similarity(graph, node1, node2)\n",
    "                \n",
    "                if score > 0:\n",
    "                    predictions.append((node1, node2, score))\n",
    "    \n",
    "    # Sort by score and return top-k\n",
    "    predictions.sort(key=lambda x: x[2], reverse=True)\n",
    "    return predictions[:k]\n\n# Make predictions\ncommon_neighbor_preds = predict_links(citation_graph, method='common_neighbors', k=10)\njaccard_preds = predict_links(citation_graph, method='jaccard', k=10)\n",
    "\nprint(\"\\nTop 10 Predicted Citations (Common Neighbors):\")\nfor paper1, paper2, score in common_neighbor_preds:\n",
    "    print(f\"  {paper1} -> {paper2}: score={score}\")\n",
    "\nprint(\"\\nTop 10 Predicted Citations (Jaccard):\")\nfor paper1, paper2, score in jaccard_preds:\n",
    "    print(f\"  {paper1} -> {paper2}: score={score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN-based Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionGNN:\n",
    "    \"\"\"Simple GNN for link prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, embedding_dim=16):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = np.random.randn(num_nodes, embedding_dim) * 0.01\n",
    "    \n",
    "    def forward(self, graph, num_layers=2):\n",
    "        \"\"\"Message passing\"\"\"\n",
    "        nodes = list(graph.nodes())\n",
    "        node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            new_embeddings = []\n",
    "            \n",
    "            for i, node in enumerate(nodes):\n",
    "                # Aggregate from predecessors (papers that cite this one)\n",
    "                predecessors = list(graph.predecessors(node))\n",
    "                \n",
    "                if predecessors:\n",
    "                    pred_embeddings = [self.embeddings[node_to_idx[p]] for p in predecessors]\n",
    "                    aggregated = np.mean(pred_embeddings, axis=0)\n",
    "                else:\n",
    "                    aggregated = np.zeros(self.embedding_dim)\n",
    "                \n",
    "                # Update with nonlinearity\n",
    "                new_emb = np.tanh(self.embeddings[i] + aggregated)\n",
    "                new_embeddings.append(new_emb)\n",
    "            \n",
    "            self.embeddings = np.array(new_embeddings)\n",
    "    \n",
    "    def predict_link_probability(self, node1_idx, node2_idx):\n",
    "        \"\"\"Predict probability of link between nodes\"\"\"\n",
    "        # Use dot product of embeddings\n",
    "        emb1 = self.embeddings[node1_idx]\n",
    "        emb2 = self.embeddings[node2_idx]\n",
    "        \n",
    "        similarity = np.dot(emb1, emb2)\n",
    "        # Scale to 0-1\n",
    "        probability = 1 / (1 + np.exp(-similarity))\n",
    "        return probability\n\n# Create GNN-based predictor\nprint(\"\\nTraining Link Prediction GNN...\")\nnodes = list(citation_graph.nodes())\nnode_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "\nlp_gnn = LinkPredictionGNN(len(nodes), embedding_dim=16)\nlp_gnn.forward(citation_graph, num_layers=3)\n",
    "\nprint(\"Training complete!\")\n",
    "\n# Make GNN predictions\nprint(\"\\nTop 10 GNN-based Link Predictions:\")\ngnn_predictions = []\n",
    "\nfor node1 in nodes:\n",
    "    for node2 in nodes:\n",
    "        if node1 != node2 and not citation_graph.has_edge(node1, node2):\n",
    "            idx1 = node_to_idx[node1]\n",
    "            idx2 = node_to_idx[node2]\n",
    "            prob = lp_gnn.predict_link_probability(idx1, idx2)\n",
    "            \n",
    "            if prob > 0.3:  # Threshold\n",
    "                gnn_predictions.append((node1, node2, prob))\n",
    "\ngnn_predictions.sort(key=lambda x: x[2], reverse=True)\n",
    "\nfor node1, node2, prob in gnn_predictions[:10]:\n",
    "    print(f\"  {node1} -> {node2}: probability={prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Link Prediction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: training (70%) and test (30%)\ntrain_edges = list(citation_graph.edges())\ntest_size = int(0.3 * len(train_edges))\nnp.random.shuffle(train_edges)\n",
    "\ntest_edges = train_edges[:test_size]\ntrain_edges = train_edges[test_size:]\n",
    "\n# Create training graph (without test edges)\ntrain_graph = nx.DiGraph()\ntrain_graph.add_nodes_from(citation_graph.nodes())\ntrain_graph.add_edges_from(train_edges)\n",
    "\nprint(f\"Train set: {train_graph.number_of_edges()} edges\")\nprint(f\"Test set: {len(test_edges)} edges\")\n",
    "\n# Generate negative samples for test set\ntest_non_edges = []\nnodes = list(citation_graph.nodes())\n",
    "\nwhile len(test_non_edges) < len(test_edges):\n",
    "    node1, node2 = np.random.choice(nodes, 2, replace=False)\n",
    "    if not citation_graph.has_edge(node1, node2) and (node1, node2) not in test_non_edges:\n",
    "        test_non_edges.append((node1, node2))\n",
    "\n# Evaluate all methods\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n# Prepare test data\ntest_pairs = test_edges + test_non_edges\ntest_labels = [1] * len(test_edges) + [0] * len(test_non_edges)\n",
    "\n# Common neighbors scores\ncommon_neighbor_scores = [common_neighbors_score(train_graph, u, v) for u, v in test_pairs]\n",
    "\n# Jaccard scores\njaccard_scores = [jaccard_similarity(train_graph, u, v) for u, v in test_pairs]\n",
    "\n# GNN scores\nlp_gnn_test = LinkPredictionGNN(len(nodes), embedding_dim=16)\nlp_gnn_test.forward(train_graph, num_layers=3)\n",
    "gnn_scores = [lp_gnn_test.predict_link_probability(node_to_idx[u], node_to_idx[v]) for u, v in test_pairs]\n",
    "\nprint(\"\\n\" + \"=\"*50)\nprint(\"Link Prediction Evaluation\")\nprint(\"=\"*50)\n",
    "\n# Calculate AUC-ROC\nauc_common = roc_auc_score(test_labels, common_neighbor_scores)\nauc_jaccard = roc_auc_score(test_labels, jaccard_scores)\nauc_gnn = roc_auc_score(test_labels, gnn_scores)\n",
    "\nprint(f\"\\nAUC-ROC Score:\")\nprint(f\"  Common Neighbors: {auc_common:.3f}\")\nprint(f\"  Jaccard Similarity: {auc_jaccard:.3f}\")\nprint(f\"  GNN: {auc_gnn:.3f}\")\n",
    "\n# Calculate Average Precision\nap_common = average_precision_score(test_labels, common_neighbor_scores)\nap_jaccard = average_precision_score(test_labels, jaccard_scores)\nap_gnn = average_precision_score(test_labels, gnn_scores)\n",
    "\nprint(f\"\\nAverage Precision:\")\nprint(f\"  Common Neighbors: {ap_common:.3f}\")\nprint(f\"  Jaccard Similarity: {ap_jaccard:.3f}\")\nprint(f\"  GNN: {ap_gnn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n# AUC scores\nmethods = ['Common Neighbors', 'Jaccard Similarity', 'GNN']\nauc_scores = [auc_common, auc_jaccard, auc_gnn]\ncolors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\naxes[0].bar(methods, auc_scores, color=colors, edgecolor='black', alpha=0.7)\naxes[0].set_ylabel('AUC-ROC Score')\naxes[0].set_title('Link Prediction Performance (AUC-ROC)')\naxes[0].set_ylim([0, 1])\naxes[0].grid(axis='y', alpha=0.3)\n",
    "\n# AP scores\nap_scores = [ap_common, ap_jaccard, ap_gnn]\naxes[1].bar(methods, ap_scores, color=colors, edgecolor='black', alpha=0.7)\naxes[1].set_ylabel('Average Precision')\naxes[1].set_title('Link Prediction Performance (Average Precision)')\naxes[1].set_ylim([0, 1])\naxes[1].grid(axis='y', alpha=0.3)\n",
    "\nplt.tight_layout()\nplt.show()\n",
    "\nprint(\"\\nBest performing method:\")\nif auc_gnn > auc_jaccard and auc_gnn > auc_common:\n",
    "    print(\"  GNN outperforms traditional methods!\")\nelse:\n",
    "    print(\"  Traditional method performs better (this can happen with small datasets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercises & Challenges\n",
    "\n",
    "## Exercise 1: Heterogeneous Graph Classification\n",
    "\n",
    "Classify venues in the academic graph as \"top\" or \"not-top\" based on papers published and their citations.\n",
    "\n",
    "**Hints**:\n",
    "- Extract features from neighborhoods\n",
    "- Use meta-path based features\n",
    "- Try different aggregation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Heterogeneous graph classification\n",
    "# TODO: Implement venue classification\n",
    "# Step 1: Extract features from academic graph\n",
    "# Step 2: Create labels (e.g., venues with >5 papers = \"top\")\n",
    "# Step 3: Train a simple classifier\n",
    "# Step 4: Evaluate performance\n",
    "\nprint(\"Exercise 1: Heterogeneous Graph Classification\")\nprint(\"Your task: Classify venues as 'top' or 'not-top' based on graph structure\")\nprint()\n",
    "\n# START YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Temporal Graph Analysis\n",
    "\n",
    "Analyze how the social network evolves over time. What patterns emerge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Temporal graph evolution\n",
    "# TODO: Analyze temporal patterns in the social network\n",
    "# Questions:\n",
    "# 1. How does network density change over time?\n",
    "# 2. Which users become more central over time?\n",
    "# 3. Can you predict future edges?\n",
    "\nprint(\"Exercise 2: Temporal Graph Analysis\")\nprint(\"Your task: Analyze evolution of the social network\")\nprint()\n",
    "\n# START YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Knowledge Graph Completion\n",
    "\n",
    "Extend the TransE model and predict more complex relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Advanced knowledge graph reasoning\n",
    "# TODO: Extend the knowledge graph and improve TransE\n",
    "# Ideas:\n",
    "# 1. Add more entities and relations\n",
    "# 2. Implement DistMult (multilinear scoring)\n",
    "# 3. Add hierarchical reasoning (A is born in B, B is in C -> path reasoning)\n",
    "\nprint(\"Exercise 3: Knowledge Graph Completion\")\nprint(\"Your task: Extend the KG and improve link prediction\")\nprint()\n",
    "\n# START YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Molecular Property Optimization\n",
    "\n",
    "Use the molecular GNN to design molecules with specific properties (e.g., high solubility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Molecular design optimization\n",
    "# TODO: Design molecules with desired properties\n",
    "# Approach:\n",
    "# 1. Generate candidate molecules (modify existing ones)\n",
    "# 2. Score them with the GNN\n",
    "# 3. Find molecules optimizing a property (e.g., max solubility, min toxicity)\n",
    "# 4. Visualize the optimized molecules\n",
    "\nprint(\"Exercise 4: Molecular Property Optimization\")\nprint(\"Your task: Design molecules with desired properties\")\nprint()\n",
    "\n# START YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Recommendation System Improvements\n",
    "\n",
    "Improve the recommendation system with better features and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Recommendation system improvements\n",
    "# TODO: Enhance the recommendation model\n",
    "# Ideas:\n",
    "# 1. Add item features (category, price, etc.)\n",
    "# 2. Implement matrix factorization\n",
    "# 3. Calculate ranking metrics (NDCG, Hits@K)\n",
    "# 4. Cold-start problem: Recommend to new users\n",
    "# 5. Cross-validate with k-fold\n",
    "\nprint(\"Exercise 5: Recommendation System Improvements\")\nprint(\"Your task: Improve recommendation accuracy and robustness\")\nprint()\n",
    "\n# START YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Your Own Project\n",
    "\n",
    "Design and implement a GNN application for a domain of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: Custom GNN Project\n",
    "# Choose one:\n",
    "# 1. Disease-Gene interaction networks\n",
    "# 2. Flight route optimization\n",
    "# 3. Protein structure prediction (contact maps as graphs)\n",
    "# 4. Traffic network forecasting\n",
    "# 5. Code clone detection (functions as nodes, calls as edges)\n",
    "\nprint(\"Exercise 6: Custom GNN Application\")\nprint(\"Your task: Build a GNN application for a domain of interest\")\nprint()\nprint(\"Suggested domains:\")\nprint(\"  - Biological networks (proteins, genes)\")\nprint(\"  - Transportation networks\")\nprint(\"  - Code analysis and bug detection\")\nprint(\"  - Supply chain optimization\")\nprint(\"  - Recommendation systems for your favorite domain\")\nprint()\n",
    "\n# START YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary & Next Steps\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "1. **Heterogeneous Graphs**: Multiple node/edge types with meta-paths\n",
    "2. **Temporal Graphs**: Dynamic evolution with RNN-like updates\n",
    "3. **Knowledge Graphs**: Entity-relation facts with TransE embeddings\n",
    "4. **Molecular Property Prediction**: GNNs for chemistry\n",
    "5. **Recommendation Systems**: Bipartite graphs and collaborative filtering\n",
    "6. **Link Prediction**: Baseline and GNN methods with evaluation\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- GNNs are powerful for complex graph structures\n",
    "- Different applications require different architectural choices\n",
    "- Evaluation metrics matter: use domain-specific metrics\n",
    "- Real-world data is messy: handle sparsity, missing values, scale\n",
    "- Interpretability is important: understand model decisions\n",
    "\n",
    "## Further Learning\n",
    "\n",
    "- Implement GNNs with PyTorch Geometric for production systems\n",
    "- Explore attention mechanisms for heterogeneous graphs\n",
    "- Study equivariant networks for 3D molecular structures\n",
    "- Investigate graph transformers for large-scale applications\n",
    "- Apply to your own domain-specific problems\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Lesson 8!** You now have a comprehensive understanding of advanced GNN topics and real-world applications. The knowledge and skills you've gained here form the foundation for cutting-edge research and industry applications in graph machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
